%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                                                 %
%   HBOOK User Guide -- LaTeX Source                              %
%                                                                 %
%   Chapter 6                                                     %
%                                                                 %
%   The following external EPS files are referenced:              %
%                                                                 %
%   Editor: Michel Goossens / CN-AS                               %
%   Last Mod.: 20 Oct 1993  9:20 mg                               %
%                                                                 %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 
\Filename{H1Operations-on-Histograms}
\chapter{Operations on Histograms}
\label{HOPERFIT}

\Filename{H2Arithmetic-Operations}
\section{Arithmetic Operations}
\label{HARITHME}

\index{histogram!operations}
\index{histogram!addition}
\index{add histograms}
\index{histogram!substraction}
\index{subscract histograms}
\index{histogram!multiplication}
\index{multiply histograms}
\index{histogram!division}
\index{divide histograms}
Histograms can be added, subtracted, divided or multiplied, provided
their number of channels are the same.
 
\Shubr{HOPERA}{(ID1,CHOPER,ID2,ID3,C1,C2)}
 
\Action
Fills an histogram \Lit{I3} with values such that,
logically (operands are the bin contents)
 
\begin{verbatim}
ID3 = C1 * ID1 (OPERATION) C2 * ID2
\end{verbatim}
 
\begin{DLtt}{123456}
\item[{\rm\bf Input parameters:}]
\item[ID1,ID2] Operand histogram identifiers.
\item[CHOPER] Character variable specifying the
     kind of operation to be performed
     (\Lit{+,-,*,/});\\
     \Lit{'B'} compute binomial errors;\\
     \Lit{'E'} compute error bars on the resulting
      histograms correctly, assuming that the
      input histograms \Lit{ID1} and \Lit{ID2} are independent.\\
     For instance \Lit{/BE} will generate binomial errors for the
     division of \Lit{ID1} by \Lit{ID2}.
\item[ID3] Identifier of the histogram containing
the result after the operation.
\item[C1,C2] Multiplicative constants.
\end{DLtt}

\Remark
\begin{UL}
\item \Lit{ID1}, \Lit{ID2} and \Lit{ID3}
      must have the same number of channels.
\item If histogram \Lit{ID3} is not empty, its contents are overwritten
\item The output histogram \Lit{ID3} can be either one of the input 
      histograms \Lit{ID1} or \Lit{ID2}.
\item If histogram \Lit{ID3} does not exist, it is created
      by \Rind{HOPERA} with the same specification as for histogram \Lit{ID1}.
\item The mean value, standard deviation, etc. are calculated from the
      contents of the resulting histogram \Lit{ID3},
      unless the \Lit{'STAT'} option\Iind{STAT} is active and the
      operation is an addition or subtraction, in which case
      they are computed exactly.
\item A division by zero gives zero.
\item Negative results for bin contents in a packed histogram are
      meaningless
\item The number of entries in the resulting histogram \Lit{ID3} is set
      to the sum of the entries in histograms \Lit{ID1} and \Lit{ID2}.
\item When an operation is performed on two 1-dimensional histograms with
      the sum of the squares of the weights stored (option \Rind{HBARX}), the
      error on the resulting histogram is calculated
      supposing that the contents of the two input histograms \Lit{ID1} and
      \Lit{ID2} are uncorrelated.
      This is valid also for projections, slices and
      bands of 2-dimensional histograms.
\item If histogram \Lit{ID3} is packed the
      number of bits allocated per channel (cell) has to be sufficient
      to store the results.
\end{UL}
 
\newpage
\section{Statistical differences between histograms}
\label{HSTATDIF}
\index{difference! between histograms}
\index{test!Kolmogorov}

\Shubr{HDIFF}{(ID1,ID2,PROB*,CHOPT)}
 
\Action
Statistical test of compatibility in shape between
two histograms using the Kolmogorov test.
The histograms are compared and the probability that they
could come from the same parent distribution is calculated.
 
The comparison may be done between two 1-dimensional
histograms or between two 2-dimensional histograms.
\index{Kolmogorov test}
For further details on the method, see \ref{HSTATCON}
below.
 
\begin{DLtt}{123456}
\item[{\rm\bf Input parameters:}]
\item[ID1] Identifier of first histogram to be compared.
\item[ID2] Identifier of second histogram to be compared.
\item[CHOPT] A character string specifying the options desired.
\begin{DLtt}{1234}
\item['D'] Debug printout, produces a blank line and two lines of
information at each call, including the identifier numbers \Lit{ID},
the number
of events in each histogram, the value of \Lit{PROB}, and the maximum
Kolmogorov distance between the two histograms.
For 2-dimensional histograms,
there are two Kolmogorov distances (see below). If option \Lit{'N'} is
specified, there is a third line of output giving the probalility
\Lit{PROB}
for shape and normalization alone separately.
\item['F1'] Histogram \Lit{ID1} has no errors (it is a function).
\item['F2'] Histogram \Lit{ID2} has no errors (it is a function).
\item['N'] Include a comparison of the relative normalization
of the
two histograms, in addition to comparing the shapes.
The output parameter \Lit{PROB} is then
a combined confidence level taking into account absolute contents.
\item['O'] Overflow, requests that overflow bins be taken
into account (also valid for 2-dim).
\item['U'] Underflow, requests that underflow bins be taken
into account (also valid for 2-dim).
\item[{\rm\bf For 2-dimensional histograms only}]
\item['L'] Left, include X-underflows in the calculation.
\item['R'] Right, include X-overflows in the calculation.
\item['B'] Bottom, include Y-underflows in the calculation.
\item['T'] Top, include Y-overflows in the calculation.
\end{DLtt}
\item[{\rm\bf Output Parameter:}]
\item[PROB] The probability of compatibility between the two histograms.
\end{DLtt}
\Remark
\begin{UL}
\item
Options \Lit{'O'} and \Lit{'U'} can also refer to 2-dimensional
histograms, so that, for example the string \Lit{'UT'} means that
underflows in  X and Y and overflows in Y should be
included in the calculation.
\item The histograms \Lit{ID1} and \Lit{ID2} must exist and already
have been filled before the call to \Rind{HDIFF}. They must also have
identical binning (lower and upper limits as well as number of bins).
\item The probability \Lit{PROB} is returned as a number
between zero and one.
A values close to one
indicates very similar histograms, and a value near zero
means that it is very unlikely that the two arose from the same
parent distribution.
\item By default (no options selected with \Lit{CHOPT})
the comparison is done only
on the shape of the two histograms, without consideration of
the difference in number of events, and ignoring all
underflow and overflow bins.
\end{UL}
 
\subsection{Weights and Saturation}
\label{HWEIGSAT}
 
\subsubsection*{Weighted 1-dimensional histograms}
 
It is possible to compare weighted with weighted histograms,
and weighted with unweighted histograms, but only
if \Lit{HBOOK} has been instructed to maintain the necessary
information by appropriate calls (before filling) to
\Rind{HBARX}.
However it is not possible to take into account underflow
or overflow bins if the events are weighted.
 
\subsubsection*{Saturated 1-dimensional histograms}
 
If there is saturation
(more than the maximum allowed contents in one or more bins),
the probability \Lit{PROB} is calculated as if the bin contents
were exactly
at their maximum value, ignoring the saturation.
This usually will result in a higher value of \Lit{PROB} than would
be the case if the memory allowed the full contents to be stored,
but not always.
\index{saturation}
It should therefore be realized that the results of \Rind{HDIFF} are
not accurate when there is saturation, and it is the user's
responsability to avoid this condition.
 
\subsubsection*{2-dimensional histograms}
 
Routine \Rind{HDIFF} cannot work if the events are weighted,
since, in the current version of \Lit{HBOOK}, the necessary information
is not maintained.
\Rind{HDIFF} will also refuse to compare 2-dimensional histograms if
there is saturation, since it does not have enough information
in this case.
 
\subsection{Statistical Considerations}
\label{HSTATCON}
 
\subsubsection*{The Kolmogorov Test}
 
The calculations in routine \Rind{HDIFF} are based on the Kolmogorov Test
\index{Kolmogorov test}
\index{Chisquare test}
(See, e.g. \cite{bib-EADIE}, pages 269-270).
It is usually superior to the better-known Chisquare Test
for the following reasons:
\begin{UL}
\item
It does not require a minimum number of events per bin,
and in fact it is intended for unbinned data (this is discussed
below).
\item
It takes account not only of the differences between corresponding
bins, but also the sign of the difference, and in particular it is
sensitive to a sequence of consecutive deviations of the same sign.
\end{UL}
\par In discussing the Kolmogorov test, we must distinguish
between the two most important properties of any test: its
{\bf power} and the calculation of its {\bf confidence level.}
 
\subsubsection*{The Power}
 
\index{null hypothesis}
The job of a statistical test is to distinguish between a
null hypothesis (in this case: that the two histograms are
compatible) and the alternative hypothesis (in this case:
that the two are not compatible). The power of a test is defined
as the probability of rejecting the null hypothesis when the
alternative is true. In our case, the alternative is not
well-defined (it is simply the ensemble of all hypotheses
except the null) so it is not possible to tell whether one test
is more powerful than another
in general, but only with respect to certain particular
deviations from the null hypothesis.
Based on considerations such as those given above, as well as
considerable computational experience, it is generally believed
that tests like the Kolmogorov or
Smirnov-Cramer-Von-Mises
\index{Smirnov-Cramer-Von-Mises test}
(which is similar but more complicated to calculate)
are probably the most powerful for the kinds of phenomena
generally of interest to high-energy physicists.
This is especially true for two-dimensional data where
the Chisquare Test is of little practical use since it requires
either enormous amounts of data or very big bins.
 
\subsubsection*{The Confidence Level for 1-dimensional data}
 
\index{confidence level}
Using the terms introduced above, the confidence level is just
the probability of rejecting the null hypothesis when it
is in fact true. That is, if you accept the two histograms
as compatible whenever the value of \Lit{PROB} is greater than 0.05,
then truly compatible histograms should fail the test
exactly 5\% of the time.
The value of \Lit{PROB} returned by \Rind{HDIFF} is calculated such that
it will be uniformly distributed between {\it zero} and {\it one}
for compatible histograms, provided the
data are not binned (or the number of bins is very large compared
with the number of events).
Users who have access to unbinned data and wish exact confidence
levels should therefore not put their data into histograms,
but should save them in ordinary Fortran arrays and call the
routine \Rind{TKOLMO} which is being introduced into the Program Library.
On the other hand, since \Lit{HBOOK} is a convenient way of collecting
data and saving space, the routine \Rind{HDIFF} has been provided,
and we believe it is the best test for comparison even on binned
data. However, the values of \Lit{PROB} for binned data will be shifted
slightly higher than expected, depending on the effects of the
binning.
For example, when comparing two uniform distributions of 500
events in 100 bins, the values of \Lit{PROB}, instead of being
exactly uniformly distributed between {\it zero} and {\it one},
have a mean value of about 0.56.
Since we are physicists, we can apply a useful rule:
As long as the bin width is small compared with any significant
physical effect (for example the experimental resolution)
then the binning cannot have an important effect.
Therefore,
we believe that for all practical purposes, the probability value
\Lit{PROB} is calculated correctly provided the user is aware that:
\begin{UL}
\item
The value of \Lit{PROB} should not be expected to have
{\bf exactly} the correct distribution for binned data.
\item
The user is responsible for seeing to it that the bin widths are
small compared with any physical phenomena of interest.
\item
The effect of binning (if any) is always to make the value of \Lit{PROB}
slightly too big. That is, setting an acceptance criterion of
(\Lit{PROB>0.05} will assure that {\bf at most}
5\% of truly
compatible histograms are rejected, and usually somewhat less.
\end{UL}
 
\subsubsection*{The Confidence Level for Two-dimensional Data}
 
The Kolmogorov Test for 2-dimensional data is not as well
understood as for one dimension.
The basic problem is that it requires the unbinned data to be
ordered, which is easy in one dimension, but is not
well-defined
(i.e. not scale-invariant) in higher dimensions.
Paradoxically, the binning which was a nuisance in one dimension
is now very useful, since it enables us to define
an obvious ordering.
In fact there are two obvious orderings (horizontal and vertical)
which give rise to two (in general different) Kolmogorov
distance measures.
Routine \Rind{HDIFF} takes the average of the two distances
to calculate the probability value \Lit{PROB},
which gives very satisfactory results.
The precautions necessary for 1-dimensional data also apply to this case.
 
%\finalnewpage%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\Filename{H2Bin-by-bin-histogram-comparisons}
\section{Bin by bin histogram comparisons}

\Shubr{HDIFFB}{(ID1,ID2,TOL,NBINS,CHOPT,NBAD*,DIFFS*)}

\Action Compare two histograms, bin by bin. For each bin, return the
        probability that the contents are from the same distribution.  
        For details of the method see below.

        The comparison may be done between two 1-dimensional histograms,
        two 2-dimensional histograms, or between two profile histograms.

\begin{DLtt}{12345}
\item[{\rm\bf Input parameters:}]
\item[ID1]   The first histogram to be compared.
             The ``reference'' histogram in options \Lit{A} and \Lit{C}.
\item[ID2]   The second histogram to be compared.
             The ``data'' histogram in options \Lit{A} and \Lit{C}.\\
             \Lit{ID1}, \Lit{ID2} are a pair of 1-D, 2-D, or profile
             histograms booked with the same number of bins.
\item[TOL]   The tolerance for a passing the test.
             Under options \Lit{S} and \Lit{C}, \Lit{TOL}
             is a number between 0 and 1 which
             represents the smallest probability considered as an acceptable
             match. 
             \mbox{\Lit{TOL}=0.05} will cause \Lit{DIFFS} to reject the 
             bin as bad if there is less than a 5\% probability the 
             two bins came from the same distribution.
             Under option \Lit{A}, \Lit{TOL} is the degree of precision  of
             match required for the test to be considered as passed. 
             \mbox{\Lit{TOL}=2.0}
             means that a data bin differing from the reference mean by
             less than 2.0 times the reference error is compatible.
\item[NBINS] The number of bins in the comparison. For a 1-dimensional
             histogram, this is the number of bins plus 0, 1 or 2, depending
             on whether the overflow and underflow channels are included.
             For a 2-dimensional histogram, this will have the total number of
             bins plus room for overflow bins along any of the axes requested.
             For more detail, see the discussion of \Lit{DIFFS} below.
\item[CHOPT] A string allowing specification of the following options:
             \begin{DLtt}{1}
             \item[N]  Use the absolute contents of each histogram, thus
                       including the normalization of the histogram as well as 
                       its shape in the comparison.  
                       By default, for the \Lit{S} and \Lit{C} options,
                       in 1- and 2-dimensional histograms, 
                       the means are adjusted for the
                       relative numbers of entries (including any overflow or
                       underflow bins requested) in \Lit{ID1} and \Lit{ID2}.
                       No adjustment is ever made for profile histograms.
             \item[O]  Overflow, requests that overflow bins be taken into
                       account.
             \item[U]  Underflow, requests that underflow bins be taken into
                       account.
             \item[R]  Right overflow bin. For a 2-dimensional histogram, it
                       includes the X-Axis overflow bin in the comparisons.  
                       If the \Lit{O} option is used, this is automatic.
             \item[L]  Left underflow bin.  Same as above, but the X-Axis
                       underflow is used.  
                       The \Lit{U} option uses this automatically.
             \item[T]  Top overflow bin.  Same as \Lit{R}, but for the Y-Axis.
             \item[B]  Bottom underflow bin.  Option \Lit{L} for the Y-Axis.
             \item[S]  Statistical comparison. Calculates the probability that
                       both bins were produced from a distribution with the 
                       same mean. 
                       This probability is referred to in \Lit{TOL} and 
                       \Lit{DIFFS}.
             \item[C]  Compatibility test.  
                       Considers bins of the reference histogram (\Lit{ID1}) 
                       as perfectly describing the true distribution.
                       Calculates the probability that the data 
                       (from \Lit{ID2}) was produced from that distribution.   
                       For 1- or 2-dimensional histograms, the Poisson 
                       mean is deduced from \Lit{ID1}. 
                       For profile histograms, the test assumes a Gaussian 
                       with mean and standard deviation given by the \Lit{ID1}. 
                       The \Lit{C} option should be used when comparing data 
                       to a function, a well-known reference, or a
                       calibration distribution.
             \item[A]  Absolute test. Like the \Lit{C} test, except that
                       \Lit{TOL} and \Lit{DIFFS} are in terms of the number 
                       of standard deviations, rather probability.
                       The test is on the number of standard deviations by 
                       which the data from \Lit{ID2} deviates from the mean.  
                       Both the mean and the standard deviation are deduced 
                       from \Lit{ID1}.
                       Error bars must be on for this option.  
                       This forbids overflow bins, underflow bins, and 
                       2-dimensional histograms.  
                       The \Lit{A} option ignores bins with zero contents 
                       in reference histogram.
             \item[Z]  Ignores bins with zero contents in the comparison.  
                       For the \Lit{S} option, ignores bins with zero 
                       contents in either histogram.
                       For the \Lit{C} and \Lit{A} option, ignores bins 
                       with zero contents in the reference histogram.  
                       The default action is to consider all
                       bins as significant.
             \item[D]  Debug printout, dumps the critical variables in the
                       comparisons, along with indicators of its weight, etc.
                       The default (no options selected) does the \Lit{S} 
                       option (statistical comparison), ignores underflow 
                       and overflow bins, and automatically corrects for the
                       difference in entries between \Lit{ID1} and \Lit{ID2}.
             \end{DLtt}
\item[{\rm\bf Output parameters:}]
\item[NBAD*]  The number of bins failing the compatibility test according
              to the criteria defined by \Lit{TOL} and \Lit{CHOPT}.
\item[DIFFS*] An array of length the number of bins being compared, which
              gives the results of the test bin by bin (confidence levels for
              options \Lit{S} and \Lit{C}, deviations for option \Lit{A}).
              Results are passed back in the form:
              \begin{DL}{1-D}
              \item[1-D] \Lit{DIFFS(NX)} for no over or underflow or
                         \Lit{DIFFS(0:NX+1)},
                         for overflow and/or underflow.
              \item[2-D] \Lit{DIFFS(NX,NY)} or \Lit{DIFFS(0:NX+1, 0:NY+1)}.
              \end{DL}
\end{DLtt}


\subsection*{When to use \Rind{HDIFFB} instead of \Rind{HDIFF}:}

\Rind{HDIFFB} treats the histogram bins individually, while \Rind{HDIFF}
treats the histogram as a whole.  
In \Rind{HDIFF}, one is comparing the overall shapes of a
probability distribution. 
Typically, an event is entered only in one
channel, and the choice of channel depends on a measured value of a continuous
coordinate, so that it makes sense for downward fluctuations in one bin to be
considered as compensated by upward fluctuations in another bin.  
In \Rind{HDIFFB},
each bin is considered independently, except, perhaps, for an overall
normalization factor which is the sum over all bins.

Thus \Rind{HDIFFB} is appropriate when:
\begin{UL}
\item It makes sense to identify a single channel as ``bad'', 
      for example if the bin contents correspond to hits in a 
      given detector element.
\item The data is heterogeneous, for example if the contents 
      are counts versus trigger bit.
\item You have already found a discrepancy on a shape with \Rind{HDIFF} 
      and wish to focus on where disagreement is worst.
\end{UL}

A plot of hits versus detector element, where the detector elements
cover some angular range, is an example of a histogram which might 
be considered with either comparison utility.  
The choice depends on the question you wish to answer:

\begin{UL}
\item If you want to know if the angular distribution looks the same, 
      use \Rind{HDIFF}.
\item If you want a report on bad detector elements, use \Rind{HDIFFB}.
\end{UL}

\subsection{Choice of \protect\Lit{TOL}:}

If you choose 0.05 for \Lit{TOL}, you should expect 5 or so bad bins per
trial from a histogram with 100 channels.  
For monitoring, you must compromise
between the number of false messages you can tolerate (based on the total
number of channels you monitor), and the amount of data you will need to
collect to claim a channel is bad.  
In general, a somewhat smaller fraction of
channels than \Lit{TOL} will be flagged as bad, 
since for discrete distributions
(Poisson statistics), the probability is quantized.  
For example, the probability might be 0.053 for 4 entries, and 0.021 for 3.  
If \Lit{TOL}=0.05, only bins with 3 or fewer entries would be flagged as bad.

\subsection*{When to use the \protect\Lit{S} option:}

The \Lit{S} option should be used when both histograms are filled with
statistical data, for example a momentum distribution from two successive 
data runs.  
Using the \Lit{S} option when comparing data to a function or known 
reference yields poor results because it attributes errors to both histograms. 
In this case, the \Lit{C} option should be selected.

\subsection*{When to use the \Lit{C} option:}

The \Lit{C} option assumes that the reference histogram contains the
theoretically expected values with no (or negligible) errors.  
Examples might be a flat distribution hand-inserted as the expectation 
for a phi distribution, or
a long data run to be compared with shorter data runs.

\subsection*{When to use the \Lit{A} option:}

The \Lit{A} option can be used as an equivalent to the \Lit{C}
option by choosing \Lit{TOL}
in terms of standard deviations instead of probability, and returns $z$ values
in \Lit{DIFFS} for each bin.

The \Lit{A} option is intended for setting by hand absolute minima and maxima.
To restrict an efficiency between 80 and 100\%, load the reference histogram
with a mean of 0.9 (via \Rind{HPAK}) and the error bar of 0.1 (via \Rind{HPAKE}),
and use \Rind{HDIFFB} with \mbox{\Lit{TOL}=1.0} and the \Lit{A} option.  
The \Lit{N} option should also be selected for this application.

\subsection*{Comparison of Weighted versus Unweighted events:}

This is in general undesirable, as it forces you into the less accurate
Gaussian approximation.  
Thus it is preferable, for example, to have unweighted Monte Carlo events 
if you need to use \Rind{HDIFFB} to compare with data.  
The only useful case is if the weighted histogram is the reference
histogram in the \Lit{C} comparison, which only makes sense if you have much
better accuracy than your data.

\subsection*{Using Profile histograms:}

The \Lit{N} option is irrelevant for profile histograms.  
The overflow/underflow options are illegal for profile histograms because 
insufficient information is stored to calculate the error bars.  
None of the test options (\Lit{S}, \Lit{C}, or \Lit{A}) check on the number
of entries in a profile histogram bin.  
(To do that, make a separate 1-dimensional histogram.)  
This has an unexpected effect when the number of entries are small.  
Bins with no entries always pass the \Lit{S} and \Lit{C} options 
(no data is compatible with any distribution), so in such
cases more bins pass than called for by \Lit{TOL}.

\subsection*{Values of \Lit{DIFFS}:}

The value of \Lit{DIFFS} may depend somewhat on the value of \Lit{TOL}
chosen, as the approximation chosen to calculate \Lit{DIFFS} depends 
on both the number of entries and on the size of \Lit{TOL} 
(how accurately \Lit{DIFFS} must be calcuated).

The \Lit{S} option sometimes returns a confidence level of 1.0 in the small
statistics calculation, i.e. there is no probability that the two numbers
came from different distributions.  
This is due to finite precision.
Values slightly higher than 1.0 will be returned when the two content
values are identical, since no statistical test could claim they came from
different distributions.

\subsection*{Other notes:}

The normalization scaling (used unless \Lit{N} option selected) is based on
channel contents for all channels requested (including overflow/underflow),
provided you select one of the overflow/underflow options.

Negative bin contents are flagged as bad bins in \Lit{S}, \Lit{C} options.

\subsection*{Statistical methods and numerical notes:}

(For simplicity, this is written as if the \Lit{N} option were in effect.)

The methods used for the \Lit{S} and \Lit{C} options are correct for
unweighted events and Poisson statistics for 1- or 2- dimensional histograms.  
Errors may result in either the \Lit{S} and \Lit{C} options for small 
tolerances if bin contents are greater than the largest allowed integer.

For the \Lit{S} option with unweighted events, the test (which is
uniformly most powerful) treats \Lit{N} = \textem{sum of the two bin contents} 
as having chosen via a binomial distibution which histogram to enter.  
The binomial parameter \Lit{p} is given by the relative normalization of the
histograms (0.5 if the total number of entries in each histogram was the same). 
For \Lit{DIFFS} values greater than \Lit{TOL}, the first two digits are correct. 
For values less than \Lit{TOL}, the two digits to the right of the first 
non-zero \Lit{TOL} digit are significant,
i.e. for \mbox{\Lit{TOL}=0.0001}, 0.000xxx are significant.  
One can force higher accuracy by setting \Lit{TOL} smaller (or even 0), 
but calculation time will increase, and warning messages will be issued.  
A Gaussian approximation is used when there are 25 or more events in each bin, 
and \mbox{\Lit{TOL}$>$0.001}.

The \Lit{C} option for unweighted events in the data histogram simply 
calculates the Poisson probability of finding $n$, the \Lit{ID2} bin value,  
given a mean equal to the bin value of \Lit{ID1}. 
A Gaussian approximation is used when the the mean is $10^{6}$ or larger, 
and \Lit{TOL} is 0.001 or larger.  
Given the expected mean, the choice of \Lit{TOL} implies bounds 
($n_{<},n_{>}$) on $n$ (i.e. $n$ within these bounds passes).  
An error occurs when the approximations used in calculating 
\Lit{DIFFS} give an incorrect value for $n_{<}$ or $n_{>}$.  
No such errors occur for mean $<10^{5}$ and \Lit{TOL} $>10^{-15}$.
The errors in $n_{<}$ or $n_{>}$ are less than 2 for mean $<10^{6}$,
\Lit{TOL} $>10^{-6}$, or
    mean $<10^{7}$, \Lit{TOL} $>10^{-5}$.  
There is a maximum $n$ beyond which \Lit{DIFFS}
returns zero, so bins with $n > n_{max}$ always fail.  For mean $<10^{7}$,
this is irrelevant for values of \Lit{TOL} $>10^{-9}$ .

For the profile histogram \Lit{S} option, \Rind{HDIFFB} calculates
the $t$ test probability that both bin means were produced from a population 
with the same mean.  
The \Lit{C} option calculates the probability of finding the
value in \Lit{ID1} given a Gaussian with $\mu$ and $\sigma$ given by the
\Lit{ID2} contents.
Small numbers of entries for either test give \Lit{DIFFS} values which are
too large, and \Rind{HDIFFB} will reject too many events in profile histograms.

For weighted events, the \Lit{S} and \Lit{C} options use a Gaussian
approximation.
This results in \Lit{DIFFS} values which are too low.  
\Rind{HDIFFB} rejects too many bins for weighted events, 
particularly for small numbers of equivalent events.


\subsection*{Error messages of \Rind{HDIFFB}:}

\newcommand{\erritem}[1]{\item[\underline{\tt#1}]\mbox{}\\}
\begin{DLtt}{xx}
\erritem{Warning: Zero tolerance}
    The passed value \Lit{TOL} is less than or equal to 0. 
    \Lit{TOL}=0. can be
    used to force highest accuracy in the \Lit{S} option.
\erritem{Warning: Only one comparison at a time, please.}
    More than one type of comparison was selected.  
    Only one of options \Lit{S}, \Lit{C}, and \Lit{A} may be used. 
    The default \Lit{S} option will be used.
\erritem{Warning: Different binning.}
    The \Lit{XMIN} values for a 1-dimensional histogram or the 
    \Lit{XMIN} and/or \Lit{YMIN}
    values on a 2-dimensional histogram are different.  
    This may give inaccurate results.
\erritem{Warning: Weighted or saturated events in 2-dimensions.}
    HBOOK does not compute error bars for two dimensional histograms, thus
    weighted event are not allowed, and \Rind{HDIFFB} can not compute 
    the correct statistics.  
    An answer is still given, but it is probably not right.
    The only reliable case is a weighted 2-dimenension histogram as the
    reference histogram for the \Lit{C} option.
\erritem{Sum of histogram contents is zero!}
    The sum of the content bins is zero.
\erritem{Histograms must be the same dimension.}
    A 1-dimensional and a 2-dimensional histogram have been specified.  In
    order for the routine to work, both must be the same dimensionality.
\erritem{Both histograms must be the standard or profile type.}
    Two different types of histograms have been specified.  
    Both must be profile or non-profile.  
    You cannot mix types.
\erritem{Not enough bins in DIFFS to hold result.}
    The parameter \Lit{NBINS} is less that the number of bins in 
    the histograms.
\erritem{Number of channels is different.}
    The number of channels in the two histograms to compare are different. 
    They must be the same before the routine will process the data.
\erritem{U/O/L/R/T/B Option with weighted events.}
    HBOOK does not compute an error bar for over-/under-flow bins, 
    thus it may not be used with weighted events.
\erritem{U/O/L/R/T/B Option with profile histograms.}
    HBOOK  does not compute an error bar for over-/under-flow bins, 
    thus it may not be used with profile histograms.
\erritem{Weighted options and no HBARX.}
    The user had not told HBOOK to figure the error bars for the histograms.
    Therefore, the operations will not be valid.
\erritem{A-option with no error bars on reference histogram.}
    The user has not told HBOOK to compute error bars for the reference
    histogram. 
    This error is also returned when the user attempts to select
    the \Lit{A} option to compare 2-dimensional histograms.
\end{DLtt}

\subsection*{Statistical comments:}

The methods used for the \Lit{S} and \Lit{C} mode are correct for unweighted events and
Poisson statistics for one or two-dimensional histograms.  
For weighted events,
a Gaussian approximation is used, which results in \Lit{DIFFS} values which are
too low when there are fewer than 25 or so ``equivalent events'' (defined
under \Rind{HSTATI}) per bin.  
This is caused by either few entries or by wide fluctuation in weights.  
The result is that \Rind{HDIFFB} rejects to many bins in this case.

Comparisons for profile histograms assume Gaussian statistics
for the \Lit{S} and \Lit{C} mode comparisons of the channel mean.  
Fewer that 25 or so events will result in \Lit{DIFFS} values which are too large.  
The result is that \Rind{HDIFFB} rejects too many event in these low statistic cases.

% Local Variables: 
% mode: latex
% TeX-master: "hboomain"
% End: 
