\documentstyle[12pt,epsfig]{seminar}
\renewcommand{\SlideColours}[2]{%
  \renewcommand{\SlideFront}[1]{\color{#1}}%
    \slideframe%
    [\psset{fillcolor=#2,fillstyle=solid}]{scplain}%
}
\begin{document}

%\SlideColours{White}{Blue}
\blackandwhite
\begin{slide}

\begin{center}Introduction\end{center}

The FATMEN package was designed to handle the needs
of the LEP experiments in terms of data access and
management. 

This presentation describes the current status
and possible future directions.

\end{slide}

\begin{slide}
\begin{center}History\end{center}

\begin{itemize}
\item
The FATMEN committee established in 1989 at request of MEDDLE
\item
Look at automatic file and tape
management systems for the LEP experiments. 
\item
FATMEN report obtainable from the CN computer science
library.
\end{itemize}

The report proposed a solution built on a layer model,
incorporating various packages, both new and existing,
into a common framework. 
\end{slide}

\begin{slide}

\begin{center}The FATMEN layers\end{center}

\begin{itemize}
\item
The robot/operator control layer
\item
The operating specific tape interface layer
\item
The STAGE/SETUP layer
\item
The Tape Management System layer
\item
The File Catalogue layer
\end{itemize}

In addition, two further components were proposed, namely
the {\it event tag database}, cf. the event lists of Aleph
or the DAD lists of OPAL, and the production database,
cf. DBL3/OPCAL/HEPDB.

\end{slide}

\begin{slide}
\begin{center}The FATMEN implementation\end{center}

\begin{itemize}
\item
Support for new tape robot
\item
Tape Management System, imported from RAL in the UK
\item
FATMEN package - a completely new system.
\end{itemize}

Always planned to migrate to 'standard' solutions.

Close mapping between FATMEN and IEEE model
\end{slide}

\begin{slide}
\begin{itemize}
\item
First lines of the FATMEN
package were written at the end of May 1989. 
\item
Possible to successfully catalogue data in FATMEN
and access it through FATMEN on CERNVM 
on July 13th 1989. 
\item
The VMS port available in early
September 
\item
Unix version shortly after.
\end{itemize}

\end{slide}

\begin{slide}
FATMEN started far too late - LEP experiments
implemented their own ad-hoc solutions.

Conversion to FATMEN has been much slower
than desired and most CERN experiments that use FATMEN
exploit only a small fraction of its power.

\end{slide}

\begin{slide}
\begin{center}Current status\end{center}

FATMEN is currently used by DELPHI, L3, OPAL, 
CPLEAR, NA44, SMC etc.

Numerous other experiments at CERN are enabled, (including both
Altas and CMS!) but their usage of FATMEN is so low that it can
be discounted.

FATMEN is used by H1 at DESY - ZEUS are planning to write their
own system based on ORACLE.

Both of the two FNAL collider experiments, CDF and D0, use
the FATMEN package heavily. 

\end{slide}

\begin{slide}

\begin{center}Using FATMEN for data access\end{center}

\begin{itemize}
\item
Normal user needs to know precisely nothing
about FATMEN, the TMS or the local staging system
\item
Must use {\bf generic-name} 
rather than a tape identifier. 
\end{itemize}

This seems entirely natural
to people used to a file catalogue, but unfortunately
seems foreign to many CERN physicists.

The following example shows how a file may be accessed and
opened through FATMEN.
\end{slide}

\begin{slide}

\begin{small}
\begin{verbatim}

GENAME = '//CERN/DELPHI/DST/PHYS/Y93V01/SUMT/C01'
LG     = LENOCC(GENAME)
LBANK  = 0
CALL FMOPEN(GENAME(1:LG),'11',LBANK,'RF',IRC)

\end{verbatim}
\end{small}

\end{slide}

\begin{slide}
This example, originally written for a test job on CERNVM in 1989,
will work regardless of whether the file in question is on a user
mini-disk, a maxi-disk, in the IBM shared file system (SFS),
on a VMSTAGE disk, on a 3420, a 3480, an Exabyte etc.

In addition, one may run {\it exactly} the same job on a VMS system,
the SHIFT cluster, CFS, or a workstation at your home lab.

To explain the above in more detail, let us examine each of the arguments
in turn.
\end{slide}

\begin{slide}
\begin{center}The generic name - GENAME\end{center}

The generic name is used to identify a file in FATMEN. By convention,
the path starts with //CERN (or //DESY for H1 and ZEUS, //FNAL for CDF and D0 etc.).
The second component of the generic name is typically the name of the experiment,
e.g. CMS.

All further components of the name are defined by the experiment.
It is recommended that a given experiment establish and document
a convention. Examples and suggestions are given in the FATMEN manual.
\end{slide}

\begin{slide}
\begin{center}The logical name\end{center}

The second argument to the call to FMOPEN is the logical name
by which the data will be accessed. This might be a link in Unix,
e.g. fort.11, a logical name on VMS, or simply a DDNAME on VM.
If a numerical value is given, FATMEN will convert this to the
appropriate convention for FORTRAN units on the host system,
e.g. FT11F001, FOR011, fort.11 etc in the above example.
\end{slide}
\begin{slide}

\begin{center}The bank address\end{center}

The bank address is not required for simple manipulations.
It is an input-output argument, and so a variable, not
constant, must be used.

\end{slide}
\begin{slide}

\begin{center}Options\end{center}

The option R is for read access. As this is the default,
it may be omitted.

The option F instructs FATMEN to perform FZ initialisation
for the input file. This includes a call to FZFILE and
the appropriate Fortran, C, VMIO or RFIO open.

\end{slide}

\begin{slide}
\begin{center}More advanced use of FATMEN\end{center}

Typical production job:
\begin{itemize}
\item
Allocate a new tape volume
\item
Write to it
\item
Set volume READONLY
\item
Update TMS tag field and FATMEN catalogue
\end{itemize}

One may also allocate {\it space}, e.g. for high capacity 
media
\end{slide}

\begin{slide}

There are many powerful facilities in FATMEN, e.g. for data import
and export, data selection, monitoring etc. Normally, these
are only of interest to a few people per collaboration - those
directly involved in the production. These facilities are documented
in the FATMEN manual and have been presented at many conferences,
including the last 3 in the CHEP series.

(In any case, LHC data should be moved as little as possible
and hopefully data export will be a thing of the past).

\end{slide}

\begin{slide}
\begin{center}Nicknames\end{center}
A concept introduced by DELPHI, but now available for general use,
is that of {\it nicknames}. These are typically short, e.g.
{\bf DSTLEP}, but expand to a complete list.

\end{slide}

\begin{slide}
\begin{verbatim}
*-----------------------------------------*                   
:NICK.ALLD92                                                            
:GNAME.P01_ALLD/*/Y92V*/*/R                                             
:DESC.All available Delphi 1992 real data.                              
*-----------------------------------------*                   
:NICK.ALLD                                                              
:GNAME.P01_ALLD/*/*/R                                                   
:DESC.All available Delphi real data.                                   
*-----------------------------------------*                   
\end{verbatim}
\end{slide}

\begin{slide}
The above nicknames, taken from the DELPHI configuration
file, show how a simply name, such as {\bf ALLD}, can
expand to an extremely long list. At the time of writing
this list corresponds to nearly 150000 files in 250 directories - all
of which can be referenced by the nickname ALLD.
\end{slide}

\begin{slide}
\begin{center}The search for the TOP quark\end{center}

After a report in the Economist concerning the potential discovery
of the top quark by CDF, I logged on to the CDF VAXcluster and
browsed through their catalogue.

The following output is a partial log of that session.

\end{slide}

\begin{slide}
\begin{footnotesize}
\begin{verbatim}

Node FNALE of the FNALD VAX Cluster - Authorized Personnel Only

Username: JAMIE
Password: 

  VAX/VMS version V5.5-2 on node FNALE - Authorized Use Only

    Last interactive login on Thursday, 31-MAR-1994 04:53
    Last non-interactive login on Thursday, 22-OCT-1992 03:14
System login initiated at 05:14:37.
System login completed at 05:14:45, JAMIE's LOGIN.COM now executing.

CDF FATMEN setup...
CDF VAXTAP setup...

\end{verbatim}
\end{footnotesize}
\end{slide}

\begin{slide}

\begin{footnotesize}
\begin{verbatim}

$FM
>>> executing CDFK$ROOT1:[FATMEN.DEVELOPMENT.LIBRARY]FATGRP.KUMAC;1
FMINIT.  Initialisation of FATMEN package
FATMEN   1.85/01 931021 08:50 CERN PROGRAM LIBRARY FATMEN=Q123
         This version created on       931022 at         1317
Current Working Directory = //FNAL/CDF
FM> ld
List of subdirectories...
//FNAL/CDF/FATMEN
//FNAL/CDF/TOP
//FNAL/CDF/USERS
//FNAL/CDF/EXOTIC
//FNAL/CDF/QCD
//FNAL/CDF/PROD
...
\end{verbatim}
\end{footnotesize}
\end{slide}
\begin{slide}
\begin{footnotesize}
\begin{verbatim}
FM>  cd top
Current Working Directory = //FNAL/CDF/TOP
FM> ld
List of subdirectories...
//FNAL/CDF/TOP/DILEPTON
//FNAL/CDF/TOP/LEPTON
//FNAL/CDF/TOP/HADRON
Total of 3 subdirectories of which 3 match
\end{verbatim}
\end{footnotesize}
\end{slide}

\begin{slide}
\begin{footnotesize}
\begin{verbatim}
Current Directory = //FNAL/CDF/TOP/DILEPTON/DST/SVX
FM> ls -w
Directory ://FNAL/CDF/TOP/DILEPTON/DST/SVX
LEP_T37280AB LEP_T37278AA LEP_T37279AA LEP_S37058AA 
LEP_R37058AA LEP_R36344AB LEP_R37052AB LEP_R36344AC
...
LEP_S37274AB LEP_T37272AB LEP_T37276AA LEP_T37275AA
LEP_T37277AB LEP_R37282AA LEP_T37280AA 
Total of    117 files in      1 directory  
FM> exit
FMEND. Terminating FATMEN package
$ logout
  JAMIE        logged out at 31-MAR-1994 05:17:16.51
\end{verbatim}
\end{footnotesize}
\end{slide}
\begin{slide}
\begin{center}FATMEN and the future\end{center}

\begin{itemize}
\item
Bring reliability of Unix version up to that of CERNVM
\item
(All development on Unix for many years)
\item
Some problems with interface to stagein, TMS etc.
\item
Hope DSCM will improve this
\end{itemize}

All SHIFT s/w should become part of CERN Program Library
\end{slide}

\begin{slide}
\begin{footnotesize}
\begin{verbatim}

fmln //CERN/NA44/RAWD/1991/PROT/450GEV/PB/HOR-3/RUN0631 3

\end{verbatim}
\end{footnotesize}

\begin{itemize}
\item
fort.3 (or appropriate link e.g. ftn03 on HP/UX) would
point to staged file (or local disk copy)
\item
Less powerful than Fortran interface
\item
FZFILE cannot be performed
\end{itemize}
\end{slide}

\begin{slide}
\begin{center}HEP data management in the LHC era\end{center}

Despite the vast amount of data that is expected to be
produced by LHC, there are good reasons to believe that
the actual storage of this data will not be a problem - at
least technologically. Less clear is whether the software
will advance sufficiently to cope with the increased demands.

\end{slide}

\begin{slide}
\begin{center}Dynamic hierarchies\end{center}

\begin{itemize}
\item
{\it Dynamic hierarchies} are a generalisation of the concept
of {\it storage classes} introduced by IBM in the 1970s. 
\item
Despite the obvious advantages for HEP, this concept is still not
used
\item
RAWDATA, DSTs, Ntuples etc. have completely different access
patterns
\item
RAWDATA file should {\it not} be copied into ROBOT by FITMEN
\end{itemize}

\end{slide}

\begin{slide}
\begin{center}Graceful retirement\end{center}

\begin{itemize}
\item
Permits data to be moved transparently from old
to new media
\item
First implemented in Los Alamos Common File System
\item
Expect many different types of medium during
lifetime of LHC
\end{itemize}
\end{slide}

\begin{slide}
\begin{center}Intelligent caching\end{center}

Interesting work on caching has been performed at the
University of Michigan as part of the IFS project.
This work suggests that intelligent caching algorithms
can greatly benefit performance. Although it is too
early to decide exactly what algorithms are required,
it is clear that sophisticated caching techniques
will be required to make best use of a complex and
expensive storage hierarchy.

\end{slide}

\begin{slide}
\begin{center}Multiple views\end{center}

Experience with FATMEN has shown that general purpose
routines for searching and listing the database are significantly
slower than dedicated ones. By optimising the query, one can
obtain dramatic performance improvements. For example, a listing
program written by L3, which took more than 26 hours to run, 
was reduced to a mere 90 seconds by eliminating all possible overhead.

Whilst it is essential to provide the general routines, it is 
important to provide special purpose routines for certain production
queries.

A well-known operation that is poorly handled by FATMEN is finding
which files are on a given volume.~\footnote{This is an unsolved
problem also in the IEEE MSS reference model.} A simple and scalable
solution is to maintain multiple databases, each optimised for 
certain types of query. However, it is essential that all database
updates pass through a single interface.

\end{slide}

\begin{slide}
We already have three distinct types of queries:

\begin{itemize}
\item
Volume based
\item
File based
\item
Event based
\end{itemize}

In addition, the file based queries can be broken down into
various categories, including run and  energy based queries.

\end{slide}

\begin{slide}
\begin{center}Hierarchical storage management and network file systems\end{center}

Although hierarchical storage management software will be an
important and essential component of any future data management
facility, it will not be sufficient for our needs, even
when combined with transparent network access, as is provided
by AFS or DFS.

There are two main arguments against such a solution, described
in more detail below.
\end{slide}

\begin{slide}
\begin{center}File system limitations\end{center}

A major problem with HSM systems is that all data must flow
through the disk cache - often part of a standard file system (cf.
Lachman Transmigrator). This is clearly inappropriate for rawdata,
particularly when acquired at the rate expected at LHC. Although
a disk cache may be used for various purposes, this should be
seperate from the normal file system for obvious reasons.

The Los Alamos Common File System, designed in the 1970s, already
provided a 'fast path' to offline storage for certain types of
data, principally rawdata. The much touted Unitree still does not
provide this facility, even though it is high on the list of 
requests.

{\bf It should be possible to write data directly to its ultimate destination
in the storage hierarchy. In addition, read access should not imply
that the data is copied to the highest level in the hierarchy.}

The inodes of migrated files still remain on disk. Although it is not
clear how many files will exist for a given experiment in the LHC era,
scaling D0 numbers by a factor of 3 gives the order of $10^9$.

\end{slide}

\begin{slide}
\begin{center}Performance, scalability and meta data\end{center}

As can already be seen from AFS, the performance of a network
file system can be poor, particularly when remote servers
are involved. A typical operation in FATMEN is a directory
listing, or a search of an entire tree. Such operations,
which can already be slow in FATMEN, would be impossible
in a normal, let alone network, file system.

Similarly, there are clearly scaling problems when dealing
with the number of files required for a future HEP experiment.

Finally, meta data is not catered for in Unix (or most other)
file systems. The limited amount of meta data that can be
stored in FATMEN (roughly 512 bytes - half that used by ADSM)
has proved extremely useful. In particular, information on how
the data should be accessed, the last access and and time
and use count are stored. Any future system must support at
least the level of meta data as is currently supported by FATMEN.
\end{slide}

\begin{slide}
\begin{center}A hybrid solution\end{center}

Although one may hope that HEP grown solutions such as
STAGE, TMS etc. will be replacable by commercial or
standard offerings on the LHC time scale, the upper
layers of the FATMEN model will still be required.

In particular, the following components will be essential.

\begin{itemize}
\item
A database for storing calibration constants, detector
geometry and other production information. This role is
currently fulfilled by HEPDB.
\item
An event catalogue permitting direct access to any event
by its characteristics.
\item
A meta data catalogue providing transparent access to data
regardless of the underlying system. This could be a hierarchical
storage management system for much of the data, e.g. all except
the rawdata, and an archive facility for the rawdata.
\end{itemize}
\end{slide}

\end{document}
