%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                                                 %
%   LEAMAX - Reference Manual -- LaTeX Source                     %
%                                                                 %
%     Constrained Non-Linear Least Squares and Maximum            %
%     Likelihood Estimation                                       %
%                                                                 %
%   Files referenced: none                                        %
%                                                                 %
%   To run, you need the CERN style cernman.sty                   %
%                                                                 %
%   Editor: Michel Goossens / CN-AS                               %
%   Last Mod.:  8 April 1993  19:00  mg                           %
%                                                                 %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentstyle[11pt,epsfig]{cernman}
\romanfont{times}
\PScommands% Initialize PS boxes
\newmathalphabet*{\mathtt}{cmtt}{m}{n}
\newmathalphabet*{\mathbf}{cmr}{b}{n}
\newenvironment{Listing}%
  {\begin{XMPt}{Output Generated}\footnotesize}{\end{XMPt}}
\begin{document}

\setcounter{secnumdepth}{3}
\setcounter{tocdepth}{2}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%    Tile page                                                     %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\def\Ptitle#1{\special{ps: /Printstring (#1) def}
\epsfbox{/user/goossens/cnasall/cnastit.eps}}
 
\begin{titlepage}
\vspace*{-23mm}
\mbox{\epsfig{file=/usr/local/lib/tex/ps/cern15.eps,height=30mm}}
\hfill
\raise8mm\hbox{\Large\bf CERN Program Library Long Writeup D501}
\hfill\mbox{}
\begin{center}
\mbox{}\\[10mm]
\mbox{\Ptitle{LEAMAX}}\\[1cm]
{\LARGE Constrained Non-Linear Least Squares}\\[5mm]
{\LARGE and Maximum Likelihood Estimation}\\[15mm]
{\LARGE Reference Manual}\\[2cm]
{\LARGE Version 93.1 (April 1993)}\\[3cm]
{\Large Application Software Group}\\[1cm]
{\Large Computing and Networks Division}\\[2cm]
\end{center}
\vfill
\begin{center}\Large CERN Geneva, Switzerland\end{center}
\end{titlepage}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%    Copyright  page                                               %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\thispagestyle{empty}
\framebox[\textwidth][t]{\hfill\begin{minipage}{0.93\textwidth}%
\vspace*{3mm}\begin{center}\Large\bf Copyright Notice\end{center}
\parskip\baselineskip
{\bf LEAMAX -- Constrained Non-Linear Least Squares and Maximum Likelihood Estimation}
 
CERN Program Library Entry {\bf D501}
 
\copyright{} Copyright CERN, Geneva 1993
 
Copyright and any other appropriate legal protection of these
computer programs and associated documentation reserved in all
countries of the world.
 
These programs or documentation may not be reproduced by any
method without prior written consent of the Director-General
of CERN or his delegate.
 
Permission for the usage of any programs described herein is
granted apriori to those scientific institutes associated with
the CERN experimental program or with whom CERN has concluded
a scientific collaboration agreement.
 
Requests for information should be addressed to:
\vspace*{-.5\baselineskip}
\begin{center}
\tt\begin{tabular}{l}
CERN Program Library Office              \\
CERN-CN Division                         \\
CH-1211 Geneva 23                        \\
Switzerland                              \\
Tel.      +41 22 767 4951                \\
Fax.      +41 22 767 7155                \\
Bitnet:   CERNLIB@CERNVM                 \\
DECnet:   VXCERN::CERNLIB (node 22.190)  \\
Internet: CERNLIB@CERNVM.CERN.CH
\end{tabular}
\end{center}
\vspace*{2mm}
\end{minipage}\hfill}%end of minipage in framebox
\vspace{6mm}
 
{\bf Trademark notice: All trademarks appearing in this guide are acknowledged as such.}
\vfill
\begin{tabular}{l@{\quad}l@{\quad}>{\tt}l}
{\em Contact Person\/}:        & K.S. K\"olbig /CN   & (KSK\atsign CERNVM.CERN.CH)\\[1mm]
{\em Technical Realization\/}: & Michel Goossens /CN & (GOOSSENS\atsign CERNVM.CERN.CH)\\[1cm]
{\em First Edition -- April 1993}
\end{tabular}
\newpage
 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%    Introductory material                                         %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\pagenumbering{roman}
\setcounter{page}{1}

This program was developed by W.~M\"onch (Bergakademie Freiberg, Germany)
when spending six months at CERN during 1992/3.
An earlier version had been written by B.~Schorr (CERN).

The present document was produced using \LaTeX{}
with the \Lit{cernman} style file, developed at CERN. 
A compressed PostScript file \Lit{d501l.ps.Z}, 
containing a complete printable version
of this manual, can be obtained at CERN by anonymous ftp as follows
(commands to be typed by the user are underlined):

\vspace*{3mm} 
\begin{tabular}{@{\hspace{12mm}}>{\tt}l}
\underline{ftp asis01.cern.ch}\\
Trying 128.141.201.136...\\
Connected to asis01.cern.ch.\\
220 asis01 FTP server (SunOS 4.1) ready.\\
Name (asis01:username): \underline{anonymous}\\
Password: \underline{your\_{}mailaddress}\\
ftp> \underline{cd doc/cernlib}\\
ftp> \underline{get d501l.ps.Z}\\
ftp> \underline{quit}\\
\end{tabular}

\vspace*{5mm}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%    Tables of contents ...                                        %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\tableofcontents
%  ==================== Body of text ==============================
\newpage
\thispagestyle{empty}
\mbox{}
\cleardoublepage
\pagenumbering{arabic}
\setcounter{page}{1}
\chapter{The Problem}

{\tt LEAMAX} is conceived as a tool to find an approximation to the
solution of a simple bound constrained minimization problem
\begin{eqnarray*}
\varphi(a) \to \min \,! \qquad a=(a_1,\ldots,a_n) \in \mathbf{R^n},
\hspace*{70mm} (*)
\end{eqnarray*}
\hspace{1cm} subject to bounds on the variables of the form
\begin{eqnarray*}
\underline{a}_j \le a_j \le \overline{a}_j \qquad (j=1,\ldots,n),
\end{eqnarray*}
for the following three cases of objective functions $\varphi$ which
occur very often in scientific research:
\begin{DL}{1234}
\item[{\bf (S)}]
The general non-linear least squares problem
 \begin{eqnarray*}
 \varphi_S(a) & = & \frac{1}{2} \sum_{i=1}^m\,\left[f_i(a)\right]^2
 \end{eqnarray*}
with non-linear functions
$f_i: \mathbf{R^n} \to \mathbf{R^1}$  $(i=1, \ldots ,m)$.  \\
\item[{\bf (F)}]
The least squares data fitting problem
\begin{eqnarray*}
\varphi_F (a) & = & \frac{1}{2} \sum_{i=1}^m \,
\left[ \frac{y_i - f(x_i,a)}{\sigma_i} \right]^2,
\end{eqnarray*}
with a non-linear function
$f: \mathbf{R^k} \times \mathbf{R^n} \to \mathbf{R^1}$ , a set of
observations $\{(x_1,y_1),\ldots,(x_m,y_m)\}$, \\
and their corresponding {\it weights}
$\{\sigma_1,\ldots,\sigma_m\}$ , $(x_i \in \mathbf{R^k} , \,
y_i \in \mathbf{R^1} , \, \sigma_i \in \mathbf{R^1} , \,i=1,\ldots,m)$.\\
\item[{\bf (M)}]
The maximum likelihood estimation
\begin{eqnarray*}
\varphi_M (a) & = & -\sum_{i=1}^m\,\ln( f(x_i,a)),
\end{eqnarray*}
with a non-linear function
$f: \mathbf{R^k} \times \mathbf{R^n} \to \mathbf{R^1}$  and a set of
data points $\{x_1,\ldots,x_m\}, \\
(x_i \in \mathbf{R^k}, \, i=1,\ldots,m )$.
\end{DL}

\chapter{Method}

The developed subprograms are based on ideas described by Mor\'e [1]
for finding the solution of a general non-linear least squares problem
by using the Levenberg-Marquardt algorithm. The method is completed by
an active set strategy for handling simple box constraints to the
unknown parameters.
\par
We begin with a short description of the underlying principle for
solving unbounded problems. In particular, we explain the basic
algorithm, the adaptive quadratic modeling used for the objective
function $\varphi(a)$ in the three cases {\bf (S)}, {\bf (F)}, {\bf (M)},
and finally, the solution of the quadratic problem.
\par
We also give information on the minimization which is subject to
bounds on the variables and the approximative calculation of the
covariance matrix, and some numerical tests.
\par
For general information on
the numerical solution of nonlinear least squares and data fitting
problems see the monograph of Bj\"orck [3]. For the statistical
background of maximum likelihood estmations, see Eadie et al. [7].

\section{Solution of Unconstrained Problems}

{\bf Basic Algorithm}
\par
{\bf Step 0 :} Choose an initial guess $a^0 \in \mathbf{R^n}$ .\\
{\bf Step 1 :} Approximate $\varphi$ by a quadratic model:
\begin{eqnarray*}
\varphi(a) \approx \tilde{\varphi}(a):= \varphi(a^0)+g_0^{\mathbf{T}}\,
(a-a^0)+ \frac{1}{2}(a-a^0)^{\mathbf{T}} \, H_0 \, (a-a^0)
\end{eqnarray*}
with an approximation $g_0 \in \mathbf{R^n}$ to the gradient
$g(a^0) \in \mathbf{R^n}$
of $\varphi$ :
\begin{eqnarray*}
g_0 \approx g(a^0)= \left(\, \frac{\partial \varphi}
{\partial a_j}(a^0) \,\right)_{j=1,\ldots,n} \, ,
\end{eqnarray*}
and an approximation $H_0 \in L(\mathbf{R^n},\mathbf{R^n})$ to the
Hessian matrix $H(a^0)  \in L(\mathbf{R^n},\mathbf{R^n})$ of $\varphi$ :
\begin{eqnarray*}
H_0 \approx H(a^0) = \left(\, \frac{\partial^2 \varphi_S}
{\partial a_j \partial a_l} (a^0) \, \right)_{j,l=1,\ldots,n} \, .
\end{eqnarray*}
{\bf Step 2 :}
Compute the solution $p^0 \in \mathbf{R^n}$ of the linear system of
equations \quad $H_0\,p^0 = -g_0$.
\par
{\bf Step 3 :} Set \quad $a^1 = a^0 + p^0$.
\par
Repeat the above {\bf Steps 1} to {\bf 3} with $a^1$ instead of $a^0$
and so on until a suitable stopping criterion is fulfilled.
\par
{\bf Remark:} The gradient $\tilde{g}(a)$ of $\tilde{\varphi}(a)$
vanishes at $a=a^0+p^0$ with the solution $p^0$ of {\bf Step 2},
i.e. $a=a^0+p^0$ is a local minimum point of the quadratic
approximation $\tilde{\varphi}$ to $\varphi$.
\par
It is necessary to compute approximations to the gradient and to the
Hessian matrix of $\varphi$ in each iteration step.
\par
{\bf Computing of Approximations to the Derivatives of First and Second
Order}
\begin{DL}{1234}
\item[{\bf (S)}] In this case the first and second derivatives are
\begin{eqnarray*}
\frac{\partial \varphi_S}{\partial a_j} (a) & = & \sum_{i=1}^m f_i(a)
\frac{\partial f_i}{\partial a_j}(a) \qquad (j=1,\ldots,n), \\
\frac{\partial^2 \varphi_S}{\partial a_j \partial a_l} (a) & = &
\sum_{i=1}^m  \: \left[ \frac{\partial f_i}{\partial a_j} (a)
\frac{\partial f_i}{\partial a_l} (a) + f_i(a)
\frac{\partial^2 f_i}{\partial a_j \partial a_l} (a) \right],
\qquad  (j,l=1,\ldots,n).
\end{eqnarray*}
The {\tt LEAMAX} package uses an approximation to the second
derivative of $\varphi$ which neglects the second term
of the Hessian matrix,
which involves second derivatives of $f_i$. This is a commonly
used technique especially for problems with small residuals. Let
\begin{eqnarray*}
  b & := & F(a) \ := \ \left( \begin{array}{c}
  f_1(a) \\ \vdots \\ f_m(a) \end{array} \right) \in \mathbf{R^m}\,,
  \\[3mm]
  J(a) & := & DF(a) \ := \ \left( \begin{array}{c} \displaystyle
  \frac{\partial f_1}{\partial a_1}(a) \cdots
  \frac{\partial f_1}{\partial a_n}(a) \\
  \displaystyle \vdots  \hspace{15mm} \vdots \\ \displaystyle
  \frac{\partial f_m}{\partial a_1}(a) \cdots
  \frac{\partial f_m}{\partial a_n}(a) \end{array} \right)
  \in L(\mathbf{R^n},\mathbf{R^m})
\end{eqnarray*}
where $J(a)=DF(a)$ is the Jacobian matrix of $F$ at $a$.
\par
Then
\begin{eqnarray*}
 g(a) & = & J(a)^{\mathbf{T}}\,b \ = \ DF(a)^{\mathbf{T}}\,F(a), \\
 H(a) & \approx & J(a)^{\mathbf{T}}\,
 J(a) \ = \ DF(a)^{\mathbf{T}}\,DF(a).
\end{eqnarray*}
Further, denote by $J_0$ the Jacobian $J(a^0)$ or an appropriate
approximation by divided differences to $J(a^0)$, then the
approximations
\begin{eqnarray*}
g_0 & := & J_0^{\mathbf{T}}\,b \ \approx \ g(a^0), \qquad
H_0 \ := \ J_0^{\mathbf{T}}\,J_0 \ \approx \ H(a^0)
\end{eqnarray*}
to the gradient $g(a^0)$ and the Hessian $H(a^0)$, respectively, are
used in {\bf Step 2} of the algorithm.
The calculation of $F(a)$ and of the partial derivatives
$DF(a)$ (if the latter is not computed by approximation) has to
be performed by a user-supplied subroutine subprogram.
\item[{\bf (F)}]
The function $\varphi_F$ is a special case of the more general function
$\varphi_S$ with
\begin{eqnarray*}
f_i(a) & = & \frac{y_i - f(x_i,a)}{\sigma_i} \qquad (i=1,\ldots,m)
\end{eqnarray*}
and the first derivatives
\begin{eqnarray*}
\frac{\partial f_i}{\partial a_j}(a) & = & -\frac{1}{\sigma_i}
\frac{\partial f}{\partial a_j}(x_i,a),
\qquad (i=1,\ldots,m;\,j=1,\ldots,n).
\end{eqnarray*}
The calculation of $f(.,a)$ and of the partial derivatives
$\partial f(.,a)/\partial a_j\, (j=1,\ldots,n)$ (if the latter are not
computed by approximation) has to
be performed by a user-supplied subroutine subprogram.
\item[{\bf (M)}]
In this case the objective function and their first and second
derivatives are
\begin{eqnarray*}
\varphi_M(a) & = & -\sum_{i=1}^m\:\ln(f_i(a))\:,
\qquad f_i(a) \ := \ f(x_i,a),\qquad (i=1,\ldots,m), \\[2mm]
\frac{\partial \varphi_M}{\partial a_j}(a) & = &-\sum_{i=1}^m
\frac{1}{f_i(a)} \cdot
\frac{\partial f_i}{\partial a_j}(a), \qquad (j=1,\ldots,n), \\[2mm]
\frac{\partial^2 \varphi_M}{\partial a_j \partial a_l} (a) & = &
\sum_{i=1}^m \frac{1}{[f_i(a)]^2}
\left[ \frac{\partial f_i}{\partial a_j} (a)
\frac{\partial f_i}{\partial a_l} (a) - f_i(a)
\frac{\partial^2 f_i}{\partial a_j \partial a_l} (a) \right], \quad
(j,l=1,\ldots,n)
\end{eqnarray*}
As in the cases {\bf (S)} and {\bf (M)}, the approximation to the second
derivative in {\tt LEAMAX} neglects the
second term of the Hessian matrix,
which involves second derivatives of $f_i$.
\par
Let
\begin{eqnarray*}
 b & := & -\,\left(\begin{array}{c}
 1 \\ \vdots \\ 1 \end{array} \right) \in \mathbf{R^m}, \qquad
 F(a) \ := \ \left(\begin{array}{c}
 \ln(f_1(a)) \\ \vdots \\ \ln(f_m(a)) \end{array} \right) \in
 \mathbf{R^m},
\end{eqnarray*}
and the Jacobian of $F$ at $a$
\begin{eqnarray*}
  J(a) & := & DF(a) \ := \ \left(\begin{array}{c} \displaystyle
  \frac{1}{f_1(a)} \cdot \frac{\partial f_1}{\partial a_1}(a) \cdots
  \frac{1}{f_1(a)} \cdot \frac{\partial f_1}{\partial a_n}(a) \\
  \displaystyle \vdots  \hspace{30mm} \vdots \\ \displaystyle
  \frac{1}{f_m(a)} \cdot \frac{\partial f_m}{\partial a_1}(a) \cdots
  \frac{1}{f_m(a)} \cdot \frac{\partial f_m}{\partial a_n}(a)
  \end{array} \right) \in L(\mathbf{R^n},\mathbf{R^m}).
\end{eqnarray*}
Then
\begin{eqnarray*}
 g(a) & = & J(a)^{\mathbf{T}}\,b, \qquad
 H(a) \ \approx \ J(a)^{\mathbf{T}}\,J(a).
\end{eqnarray*}
Further, denote by $J_0$ the Jacobian $J(a^0)$ or an appropriate
divided differences approximation to $J(a^0)$, then the following
approximations
\begin{eqnarray*}
 g_0 & :=& J_0^{\mathbf{T}}\,b \ \approx \ g(a^0), \qquad
 H_0 \ :=\ J_0^{\mathbf{T}}\,J_0 \ \approx \ H(a^0)
\end{eqnarray*}
to the gradient $g(a^0)$ and the Hessian $H(a^0)$, respectively, are
used in {\bf Step 2} of the algorithm.
\par
The calculation of $f(.,a)$ and of the partial derivatives
$\partial f(\cdot,a)/\partial a_j\,(j=1,\ldots ,n)$
(if the latter is not computed by approximation) has to
be performed by a user-supplied subroutine subprogram.
\end{DL}
{\bf Solution of the System of Linear Equations in Step 2}
\par
In all three cases {\bf (S)}, {\bf (F)} and {\bf (M)},
the linear system of equations in {\bf Step 2}
\begin{eqnarray*}
 J^{\mathbf{T}}\,J\,p & = & -J^{\mathbf{T}}\,b
 \qquad \mathrm{or} \qquad J^{\mathbf{T}}\,(b+J\,p) \ = \ 0,
\end{eqnarray*}
which is the system of normal equations to the linear least squares
problem
\begin{eqnarray*}
\psi (p) & := & \parallel b+J\,p \parallel\,\to\, \min \,!
\qquad (p \in \mathbf{R^n})
\end{eqnarray*}
where $\parallel \cdot \parallel$ denotes the $l_2$ vector norm,
has to be solved in each iteration step.
In nearly rank deficient cases the above problem is replaced by the
following restricted linear least squares problem
\begin{eqnarray*}
\psi(p) & := & \parallel b+J\,p \parallel\, \to\, \min\,! \quad
(p \in \mathbf{R^n})  \qquad \mathrm{subject}\,\mathrm{to} \qquad
\parallel D \, p \parallel \,\leq \Delta,
\end{eqnarray*}
where $D$ is a diagonal matrix which takes into account the
scaling of the problem. The details of choosing suitable scaling matrices
$D$ and bounds $\Delta$ are described in Mor\'e [1]. He also gives
further details of the implementation of the above basic algorithm.

\newpage
\section{Minimization subject to Bounds on the Variables}

If the unconstrained minimization of the objective function $\varphi$
leads to a local minimum point which is not acceptable to the given
problem it is often necessary to take constraints to the unknown
parameters $a$ into account.
We give a short description how box constraints
\begin{eqnarray*}
\underline{a}_j \le a_j \le \overline{a}_j \qquad (j=1,\ldots,n)
\end{eqnarray*}
are handled in {\tt LEAMAX} .
 
Variables which are not on their bounds are called free variables.
Let $p$ denote the number of free variables at a solution
point $a^*$ of problem ($*$), let
$P \in L(\mathbf{R^n},\mathbf{R^p})$ denote the matrix
whose colums are the colums of the identity matrix corresponding to the
free variables at $a^*$, and let
\begin{eqnarray*}
 I & := & \{\:j\;|\;1 \le j \le n, \quad
  a_j^* \ = \ \underline{a}_j \; \vee \; a_j^* = \overline{a}_j\;\}
\end{eqnarray*}
denote the set of indices of non-free variables at $a^*$.
\par
Sufficient conditions for a feasible point $a^*$ to be a
solution of the constrained problem ($*$) are as follows: \\[2mm]
\begin{tabular}{l@{\qquad}c@{\qquad}l@{\quad}l}
(i)   & $g_j^* = 0$, & if &
$\underline{a}_j < a_j^* < \overline{a}_j$, \\
(ii)  & $g_j^* > 0$, & if & $a_j^* = \underline{a}_j$, \\
(iii) & $g_j^* < 0$, & if & $a_j^* = \overline{a}_j$, \\
(iv)  & $H_P(a^*)$ & \multicolumn{2}{l}{is positive definite,}
\end{tabular}
\par
where $g^* = g(a^*) \in \mathbf{R^n}$ is the gradient of $\varphi (a)$,
and $H_P(a^*)=P^{\mathbf{T}}\,H(a^*)\,P \in L(\mathbf{R^p},\mathbf{R^p})$
is the Hessian matrix of $\varphi (a)$ with respect to the free
variables.
\par
In the case of box constraints the Lagrange multipliers corresponding to
the active constraints, i.e. to the non-free variables are given by
\begin{eqnarray*}
\lambda_j & = & g_j^*, \qquad (j \in I).
\end{eqnarray*}
The used method for handling box-constraints is an active set method.
Bounds on the variables are dealt with by fixing some of the variables
on their bounds and adjusting the remaining free variables to minimize
the function $\varphi$. By examining estimates of the
Lagrange multipliers it is possible to adjust the set of variables fixed
on their bounds so that eventually the bounds active at a solution $a^*$
should be correctly identified. \\ [5mm]
 

\newpage
\section{Calculation of an Approximation to the Covariance Matrix}
 
In the case of data fitting problems, approximations to the covariance
matrix $C$ and the standard deviations $s$ of the estimated parameters
with respect to the free variables are also
provided by {\tt LEAMAX}. \\
Let $p$ denote the number of free variables at a solution point $a^*$.
The expressions used to approximate
$C =(c_{i,j}) \in L(\mathbf{R^p},\mathbf{R^p})$
and $s=(s_j) \in \mathbf{R^p}$ are
\begin{eqnarray*}
C & = & \sigma^2\,(P^{\mathbf{T}}\,J^{\mathbf{T}}\,J\,P)^{-1}
\ \approx \ \sigma^2 \, (H_P(a^*))^{-1}
\end{eqnarray*}
with the estimated residual variance
\begin{eqnarray*}
\sigma^2 & = & \frac{1}{m-p} \sum_{i=1}^m \left[y_i-f(x_i,a)\right]^2
\end{eqnarray*}
and $s_j=c_{j,j}\,(j=1,\ldots,p)$, respectively.

\section{Examples and Numerical Tests}

{\tt LEAMAX} runs correctly for some selected problems from the
Mor\'e-Garbow-Hillstrom test set [2], for the test problem from
the User's guide to the NAG Library, Chapter E04, and for
other constrained problems.
\par
We now demonstrate the usage of {\tt DSUMSQ}, {\tt DFUNFT} and
{\tt DMAXLK} by three examples.
\begin{DL}{1234}
\item[\mathbf{(S)}]
Objective function (Powell function, $n=4,\,m=4,$ see
[2, p. 23] and [9, Chapter E04]):
$$ \varphi_S(a) \ = \ \frac{1}{2}
\left[(a_1+10 \, a_2)^2+5 \, (a_3-a_4)^2+(a_2-2 \, a_3)^4
      +10 \, (a_1-a_4)^4\right] , $$
with the constraints:
$1 \le a_1 \le 3$, $-2 \le a_2 \le 0$, $1 \le a_4 \le 3$.
Initial guess: $a^0=(3,-1,0,1)^{\mathbf{T}}$. \\
\end{DL}

\begin{XMPt}{Example of the use of {\tt DSUMSQ}}
      IMPLICIT DOUBLE PRECISION (A-H,O-Z)
      PARAMETER (NC = 4, NF = 20, MAXIT = 50)
      PARAMETER (NW = 9*NC+4*NF+2*NF*NC+3*NC*NC)
      DIMENSION A(NC),AL(NC),AU(NC),DPHI(NC),COV(NC,NC),STD(NC)
      DIMENSION IAFR(2*NC),W(NW)
      EXTERNAL SUBS
 
      DATA M,N,MODE,IPRT,EPS / 4,4,1,-5,1D-12 /
      DATA AL,AU,A / 1D0,-2D0,-1D30,1D0, 3D0,0D0,1D30,3D0, 3D0,-1D0,0D0,1D0 /
 
      CALL DSUMSQ(SUBS,M,N,NC,A,AL,AU,MODE,EPS,MAXIT,IPRT,MFR,IAFR,
     +            PHI,DPHI,COV,STD,W,NERR)
      END
      SUBROUTINE SUBS(N,A,M,F,DF,MODE,NERROR)
      IMPLICIT DOUBLE PRECISION (A-H,O-Z)
      PARAMETER (Z0 = 0, Z1 = 1, Z2 = 2, Z5 = 5, Z10 = 10)
      DIMENSION A(*),F(*),DF(M,*)
      NERROR=0
      F(1)=A(1)+Z10*A(2)
      F(2)=SQRT(Z5)*(A(3)-A(4))
      F(3)=(A(2)-Z2*A(3))**2
      F(4)=SQRT(Z10)*(A(1)-A(4))**2
      IF(MODE .EQ. 0) RETURN
      CALL DMSET(M,N,Z0,DF(1,1),DF(1,2),DF(2,1))
      DF(1,1)=Z1
      DF(1,2)=Z10
      DF(2,3)=SQRT(Z5)
      DF(2,4)=-DF(2,3)
      DF(3,2)=Z2*(A(2)-Z2*A(3))
      DF(3,3)=-Z2*DF(3,2)
      DF(4,1)=Z2*SQRT(Z10)*(A(1)-A(4))
      DF(4,4)=-DF(4,1)
      RETURN
      END
\end{XMPt}
 
\begin{Listing}
               MATHLIB PACKAGE   D501   VERSION 15.03.93
               PACKAGE LEAMAX  ****  ROUTINE DSUMSQ ****
   MINIMIZATION OF A SUM OF SQUARES   ANALYTICAL DERIVATIVES (MODE = 1)
 
INPUT  OF  DSUMSQ :
M :  4     N :  4     NC:  4    MAXIT :  50    MODE :  1    IPRT :  -5
EPS :  1.0D-12
AL :   1.00000D+00   -2.00000D+00   -1.00000D+30    1.00000D+00
AU :   3.00000D+00    0.00000D+00    1.00000D+30    3.00000D+00
A  :   3.00000D+00   -1.00000D+00    0.00000D+00    1.00000D+00
 
ITERATION
 
    PHI = VALUE OF OBJECTIVE FUNCTION          GNO = NORM OF GRADIENT
 
       ITERATION   0   PHI =  1.07500D+02      GNO =  2.29388D+02
       PARAMETER       PARAMETER         GRADIENT          LAGRANGE
         NUMBER          VALUE                            MULTIPLIER
            1          3.00000D+00      1.53000D+02      0.00000D+00
            2         -1.00000D+00     -7.20000D+01      0.00000D+00
            3          0.00000D+00     -1.00000D+00      0.00000D+00
            4          1.00000D+00     -1.55000D+02      0.00000D+00
 
       ITERATION   5   PHI =  1.24395D+00      GNO =  2.30084D-01
       PARAMETER       PARAMETER         GRADIENT          LAGRANGE
         NUMBER          VALUE                            MULTIPLIER
            1          1.16011D+00      2.29108D-01      0.00000D+00
            2         -1.01310D-01     -2.04628D-02      0.00000D+00
            3          4.02675D-01     -5.41128D-03      0.00000D+00
            4          1.00000D+00      2.90453D+00      2.90453D+00
 
      . . .
 
 END:  ITERATION  25   PHI =  1.21689D+00      GNO =  3.02266D-06
       PARAMETER       PARAMETER         GRADIENT          LAGRANGE
         NUMBER          VALUE                            MULTIPLIER
            1          1.00000D+00      1.47674D-01      1.47674D-01
            2         -8.52326D-02      1.35178D-06      0.00000D+00
            3          4.09303D-01     -2.70355D-06      0.00000D+00
            4          1.00000D+00      2.95348D+00      2.95348D+00
\end{Listing}
\newpage  
\begin{DL}{1234}
\item[\mathbf{(F)}]
Objective function (Bard function, $n=3,\,m=15,\,k=3$, see [2, p. 22]):
$$ \varphi_F(a) \ = \ \frac{1}{2} \, \sum_{i=1}^m \:
\left[y_i-\left(a_1+\frac{x_{1,i}}{x_{2,i}\,a_2+x_{3,i}\,a_3}\right)
\right]^2, $$
without constraints to the unknown parameters $a_1$, $a_2$, $a_3$. \\
The chosen data set $\{x_{1,i},x_{2,i},x_{3,i},y_i\}_{i=1,\ldots,m}$
is that of the original paper [2] with {\it weights}
$\sigma_i=1\,(i=1,\ldots,m)$.
Initial guess: $a^0=(1,1,1)^{\mathbf{T}}$. \\
\end{DL}

\begin{XMPt}{Example of the use of {\tt DFUNFT}}
      IMPLICIT DOUBLE PRECISION (A-H,O-Z)
      PARAMETER (NC = 3, NX = 3, NF = 15, MAXIT = 50)
      PARAMETER (NW = 9*NC+4*NF+2*NF*NC+3*NC*NC)
      DIMENSION X(NX,NF),Y(NF),SY(NF),A(NC),AL(NC),AU(NC),DPHI(NC)
      DIMENSION COV(NC,NC),STD(NC),IAFR(2*NC),W(NW)
      EXTERNAL SUBF
      DATA K,N,M,MODE,IPRT,EPS / 3,3,15,0,1,1D-12 /
      DATA AL,AU,A,SY / 3 * -1D30 , 3 * 1D30 , 3 * 1D0 , 15 * 1D0 /
      DATA Y / 0.14D0, 0.18D0, 0.22D0, 0.25D0, 0.29D0, 0.32D0, 0.35D0, 0.39D0,
     +         0.37D0, 0.58D0, 0.73D0, 0.96D0, 1.34D0, 2.10D0, 4.39D0 /
      DO  10 I = 1,M
      X(1,I)=I
      X(2,I)=16-I
   10 X(3,I)=MIN(X(1,I),X(2,I))
 
      CALL DFUNFT(SUBF,K,M,N,NX,NC,X,Y,SY,A,AL,AU,MODE,EPS,MAXIT,IPRT,
     +            MFR,IAFR,PHI,DPHI,COV,STD,W,NERR)
      END
      SUBROUTINE SUBF(K,X,N,A,F,DF,MODE,NERROR)
      IMPLICIT DOUBLE PRECISION (A-H,O-Z)
      DIMENSION A(*),X(*),DF(*)
      T=X(2)*A(2)+X(3)*A(3)
      IF (T .EQ. 0) THEN
       NERROR=3
      ELSE
       NERROR=0
       F=A(1)+X(1)/T
      ENDIF
      RETURN
      END
\end{XMPt}
\newpage
\begin{Listing}
   . . .
       ITERATION 0 PHI =  2.08408D+01  GNO =  4.23154D+01
       PARAMETER   PARAMETER     GRADIENT    LAGRANGE
        NUMBER       VALUE                  MULTIPLIER
          1      1.00000D+00  2.18829D+01  0.00000D+00
          2      1.00000D+00 -2.59356D+01  0.00000D+00
          3      1.00000D+00 -2.52800D+01  0.00000D+00
 
       ITERATION 1 PHI =  6.32350D-01  GNO =  4.20952D+00
       PARAMETER   PARAMETER     GRADIENT    LAGRANGE
         NUMBER      VALUE                  MULTIPLIER
          1      8.26475D-02  2.39601D+00  0.00000D+00
          2      1.18349D+00 -2.45108D+00  0.00000D+00
          3      1.66615D+00 -2.44365D+00  0.00000D+00
     . . .
  END: ITERATION 6 PHI =  4.10744D-03  GNO =  2.88843D-10
       PARAMETER   PARAMETER     GRADIENT    LAGRANGE    STANDARD
         NUMBER      VALUE                  MULTIPLIER   DEVIATION
          1      8.24106D-02 -2.29064D-10  0.00000D+00  1.23742D-02
          2      1.13304D+00  1.64057D-10  0.00000D+00  3.07900D-01
          3      2.34370D+00  6.36029D-11  0.00000D+00  2.96278D-01
\end{Listing}
\newpage 
\begin{DL}{1234}
\item[\mathbf{(M)}]
Objective function ($n=1,\,m=50,\,k=1$):
$$ \varphi_M(a) \ = \  -\sum_{i=1}^m\,\ln\:\left\{
\frac{1}{a_1\sqrt{\pi}} \, \exp \left[-\frac{1}{2}
\left(\frac{x_{1,i}-1}{a_1} \right)^2 \right] \right\},$$
with the bounds $0.3 \le a_1 \le 10$. The data points
$\{x_{1,i}\}_{i=1,\ldots,m}$ are pseudorandom-numbers.
Initial guess: $a_1^0=1$.
\end{DL}

\begin{XMPt}{Example of the use of {\tt DMAXLK}}
      IMPLICIT DOUBLE PRECISION (A-H,O-Z)
      PARAMETER (NC = 1, NX = 1, NF = 50, NW = NC*(2*NC+7), MAXIT = 50)
      DIMENSION X(NX,NF),AL(NC),AU(NC),A(NC),DPHI(NC),IAFR(NC),W(NW)
      EXTERNAL SUBM
 
      DATA K,N,M,MODE,IPRT,EPS / 1,1,50,0,5,1D-12 /
      DATA AL,AU,A / 1D-1,1D1,5D-1 /
 
      IR=13846
      DO 10 I=1,M
      IR=MOD(31416*IR+13846,46261)
   10 X(1,I)=0.5D0-IR/46260D0
 
      CALL DMAXLK(SUBM,K,M,N,NX,X,A,AL,AU,MODE,EPS,MAXIT,IPRT,
     +            MFR,IAFR,PHI,DPHI,W,NERR)
      END
      SUBROUTINE SUBM(K,X,N,A,F,DF,MODE,NERROR)
      IMPLICIT DOUBLE PRECISION (A-H,O-Z)
      DIMENSION A(*),X(*),DF(*)
      PARAMETER (PIR = 0.56418 95835 47756 287D0)
      NERROR=3
      IF(A(1) .EQ. 0) RETURN
      T=0.5D0*((X(1)-1)/A(1))**2
      F=PIR*EXP(-T)/A(1)
      IF(F .GT. 0) NERROR=0
      RETURN
      END
\end{XMPt}

\begin{Listing}
       ITERATION   0   PHI =  1.02696D+02      GNO =  3.34941D+02
       PARAMETER       PARAMETER         GRADIENT          LAGRANGE
         NUMBER          VALUE                            MULTIPLIER
            1          5.00000D-01     -3.34941D+02      0.00000D+00
 
       ITERATION   5   PHI =  5.57153D+01      GNO =  8.03750D-01
       PARAMETER       PARAMETER         GRADIENT          LAGRANGE
         NUMBER          VALUE                            MULTIPLIER
            1          1.03420D+00     -8.03750D-01      0.00000D+00
. . . . .
  END: ITERATION  15   PHI =  5.57119D+01      GNO =  3.60421D-07
       PARAMETER       PARAMETER         GRADIENT          LAGRANGE
         NUMBER          VALUE                            MULTIPLIER
            1          1.04276D+00      3.60421D-07      0.00000D+00
\end{Listing}

\chapter{Remarks on the Usage of LEAMAX}

\begin{itemize}
\item {\bf Default values.} \\
Default values have been specified in {\tt LEAMAX} for the arguments
{\tt EPS}, {\tt A}, and {\tt SY}. In particular: \\
$\mathtt{EPS = 10\, \varepsilon_0}$, if
$\mathtt{EPS \notin [\varepsilon_0,0.1]}$, where $\varepsilon_0$ is an
approximation to the {\it machine} constant $\varepsilon$,
i.e. the greatest real number with $1+\varepsilon =1$; \\
$\mathtt{A(I) = AU(I)}$ if $a_i>\overline{a}_i, \quad
 \mathtt{A(I) = AL(I)}$ if $a_i<\underline{a}_i;$ \quad
$\mathtt{SY(I)=1}$ if $\sigma_i \le 0$.
\item {\bf Automatic output.} \\
{\tt LEAMAX} offers, in addition to the initial and final
summary prints, an intermediate print option (see Examples).
\item {\bf Computation of the derivatives.} \\
The derivatives which are necessary in each step can be computed either
by a user-supplied subroutine subprogram (see Examples), which must
calculate them from their analytic expressions,
or by instructing {\tt LEAMAX} to do this internally by
finite differences. In general, the results obtained in both ways
differ only slightly.
\item {\bf Starting values.} \\
The user must supply starting values
$a^0 = (a_1^0, \ldots ,a_n^0)^{\mathbf{T}}$ for the unknown parameters
$a_1,\ldots,a_n$.  Good initial approximations to a solution point can
significantly decrease the number of iterations required to find a
solution. On the other hand, a poor initial guess may even prevent a
solution from being found.
\item {\bf Bounds on the Variables.}
It is recommended to use only constraints which are really necessary.
{\tt LEAMAX} allows that one or more of the unknowns
are treated as constants
($\underline{a}_l=\overline{a}_l=a_l=\mathrm{const.}$)
This fact allows the user to examine models with fewer parameters
without rewriting the model subprogram.
\end{itemize}

\chapter*{Bibliography}
\begin{enumerate}
\item J.J. Mor\'e: The Levenberg - Marquardt algorithm:
      Implementation and theory.
      In: Numerical Analysis, G.A.Watson (Ed.),
      Lecture Notes in Mathematics 630,
      Springer-Verlag, New York, 1977, 105-116.
\item J.J. Mor\'e, B.S. Garbow, K.E. Hillstrom:
      Testing unconstrained optimization software.
      ACM Trans. Math. Software {\bf 7} (1981), 17-41.
\item \AA. Bj\"orck: Solution of Equations in $\mathbf{R^n}$
      (Part 1: Least Squares Methods).
      In: Handbook of Numerical Analysis,
      P.G. Ciarlet, J.L. Lions (Eds.),
      North-Holland, Amsterdam, New York, Oxford,
      Tokyo, 1990, 467-636.
\item R. Fletcher:  Practical Methods of Optimization.
      John Wiley and Sons, Chichester, 2nd Edition, 1987.
\item J.E. Dennis Jr., R.B. Schnabel: Numerical Methods for
      Unconstrained Optimization and Nonlinear Equations.
      Prentice-Hall, Englewood Cliffs, NJ, 1983.
\item J.R. Donaldson, R.B. Schnabel: Computational experience with
      confidence regions and confidence intervals for non-linear
      least squares. Technometrics {\bf 29} (1987), 67-82.
\item W.T. Eadie, D. Dryard, F.E. James, M. Roos, B. Sadoulet:
      Statistical Methods in Experimental Physics.
      North-Holland, Amsterdam, London, 1971.
\item E. Anderson {\it et al}.: LAPACK - User's Guide.
      SIAM, Philadelphia, 1992.
\item NAG Library, Mark 15 - User's Guide, 1992.
\end{enumerate}
\end{document}







