<HEAD>
<TITLE> Overview of the SHIFT Architecture</TITLE>
</HEAD>
<BODY><P>
 <HR> <A NAME=tex2html109 HREF=chapter2_4.html><IMG ALIGN=MIDDLE SRC="http://asis01.cern.ch/CN/CERNTEX/icons/next_motif.gif"></A> <A NAME=tex2html107 HREF=shiftuser.html><IMG ALIGN=MIDDLE SRC="http://asis01.cern.ch/CN/CERNTEX/icons/up_motif.gif"></A> <A NAME=tex2html101 HREF=chapter2_2.html><IMG ALIGN=MIDDLE SRC="http://asis01.cern.ch/CN/CERNTEX/icons/previous_motif.gif"></A> <A NAME=tex2html111 HREF=tableofcontents2_1.html><IMG ALIGN=MIDDLE SRC="http://asis01.cern.ch/CN/CERNTEX/icons/contents_motif.gif"></A> <BR>
<B> Next:</B> <A NAME=tex2html110 HREF=chapter2_4.html> The SHIFT Configuration </A>
<B>Up:</B> <A NAME=tex2html108 HREF=shiftuser.html>No Title</A>
<B> Previous:</B> <A NAME=tex2html102 HREF=chapter2_2.html> Introduction</A>
<HR> <P>
<H1><A NAME=SECTION0030000000000000000> Overview of the SHIFT Architecture</A></H1>
<P>
The designers of <i>shift</i> were motivated by
the appearance on the market of inexpensive
processors and storage systems, using technology developed for personal
 workstations,
and which had performance characteristics comparable with those of traditional
mainframes.
<P>
The goal was to define an architecture which could be used for general purpose
High Energy Physics (HEP) computing,
could be implemented to provide systems
with an excellent price/performance ratio when compared
with mainframe solutions, and could be scaled up to provide very
 large<A NAME=tex2html2 HREF=shiftuser.foot.html#125><IMG ALIGN=MIDDLE SRC="http://asis01.cern.ch/CN/CERNTEX/icons/foot_motif.gif"></A>
integrated facilities, or down to provide a system suitable for a small
physics department.
<P>
Some important characteristics of <em>offline</em> HEP processing relevant to the
the design choices are:
<UL><LI> the volume of data which must be held on online storage (up to the order
 of <tex2html_verbatim_mark>displaymath1 bytes);
<LI> the need for access to magnetic tapes, used to store fuller information
about ``events&quot; (a few terabytes);
<LI> difficulty in finding vectorisable algorithms for a significant
fraction of the processing requirements (hard to exploit supercomputers);
<LI> inherent parallelism in much of the processing (events are largely
independent);
<LI> synchronisation of read/write file access is performed at an application
 level
(essentially: data files are not modified, rather new copies are created,
with access organised through high-level packages such as FATMEN<A NAME=tex2html3 HREF=shiftuser.foot.html#588><IMG ALIGN=MIDDLE SRC="http://asis01.cern.ch/CN/CERNTEX/icons/foot_motif.gif"></A>);
<LI> use of standard HEP packages<A NAME=tex2html4 HREF=shiftuser.foot.html#589><IMG ALIGN=MIDDLE SRC="http://asis01.cern.ch/CN/CERNTEX/icons/foot_motif.gif"></A>
for accessing data from FORTRAN programs. This makes it possible to replace the
 underlying
basic I/O system by means of application-level libraries.
</UL>The SHIFT system consists of sets of
<i>cpu servers, disk servers</i> and <i>tape servers</i>.
Files are stored in <i>pools</i>, each pool
comprising one or more Unix
file systems residing on one or more <i>shift</i> nodes.
Access to the disk pools and files is coordinated by the <i>Disk Pool
 Manager</i>.
<P>
<H1><A NAME=SECTION0031000000000000000> Scalability</A></H1>
<P>
A prime goal of the SHIFT project was to build facilities which could scale
in capacity from small systems up to several times that of the
current CERN computer center.
To achieve this, an architecture was chosen which encouraged a separation of
functionality.
Each component of the system has a specific function, CPU server,
disk server, tape server and this modular approach allows the whole facility
to be extended in a flexible manner.
<P>
The servers are interconnected by the
<em>backplane,</em>
a very fast network medium used for optimized special purpose data transfers.
TCP/IP protocols are supported across the backplane
and connection to CERN's general
purpose network infrastructure is by means of an
<em>IP router.</em>
This provides access to workstations distributed throughout CERN and
at remote institutes.
<P>
The components of SHIFT are controlled by distributed software which is
responsible for managing disk space, staging data between tape and
disk, batch job scheduling and accounting.
<P>
<H1><A NAME=SECTION0032000000000000000> Software Portablity</A></H1>
<P>
Software portablity was an important aspect of the SHIFT development for
two reasons:
<UL><LI>
It allowed flexibility in the choice of hardware platform
for each system component.
<LI>
It is important to be able to benefit
from the continuous and rapid progress being made in hardware technology.
</UL>Addition of further system types to the existing configuration is
regularly reviewed and no major difficulties are foreseen in incorporating any
Unix based systems.
<P>
<H1><A NAME=SECTION0033000000000000000> Software Components in SHIFT</A></H1>
<P>
The main software components of SHIFT are as follows.
<P>
<H2><A NAME=SECTION0033100000000000000> Disk Pool Manager</A></H2>
<P>
The SHIFT filebase comprises many Unix filesystems located
on different SHIFT server systems.
In order that users see a unified data file space,
the notion of a
<em>pool</em>
was created. A
<em>pool</em>
is a group of one or several Unix filesystems and it is at the
<em>pool</em>
level that file allocation is made by the user.
Pools can be much larger than conventional Unix filesystems
even where logical volumes are available. Pools may also
be assigned attributes. For example, a pool used for staging space
can be subject to a defined garbage collection algorithm.
<P>
The pools in SHIFT are all managed by the
<em>Disk Pool Manager.</em>
It is responsible for
disk space allocation when creating new files/directories
and it may be used to locate and delete existing files.
<P>
The interface to the
<em>Disk Pool Manager.</em>
is via Unix user commands.
The
<em>sfget</em>
command allocates a file of a given size within a specified
pool. The command returns a full path name for the file based on the
convention that all SHIFT file systems are mounted globally with NFS
on the mount point
<tt>/shift/&lt;host name&gt;</tt>
If the file requested already exists within the pool,
<em>sfget</em>
simply returns the path name without allocating space. Other commands are
provided to list, remove and manage files.
<P>
In addition,
a user-callable garbage collector has been implemented
which maintains defined levels of free space within a pool.
This is useful for physics data staging where data are copied
from tape to disk before being accessed by user programs.
<P>
<H2><A NAME=SECTION0033200000000000000> Tape Copy Scheduler</A></H2>
<P>
The tape copy scheduler organises the copying of data between <i>shift</i> disk files
and magnetic tapes. On request from a user through a <i>tpread</i> or <i>tpwrite</i>
command, it selects an appropriate tape server depending on
the device type required, location of the tape and current tape activity.
It then initiates
a tape copy using the <i>cptpdsk</i> or <i>cpdsktp</i> program on the tape server
 node. The tape copy
scheduler informs the user when the operation is complete, queues
concurrent requests for the same tape, and deals with error recovery.
The tpread and tpwrite commands are described in the SHIFT Reference Manual.
<P>
<H2><A NAME=SECTION0033300000000000000> Remote File I/O System</A></H2>
<P>
The Remote File I/O system (RFIO) provides an efficient
way of accessing remote files on SHIFT.
Remote file access is also possible using
NFS<A NAME=tex2html5 HREF=shiftuser.foot.html#590><IMG ALIGN=MIDDLE SRC="http://asis01.cern.ch/CN/CERNTEX/icons/foot_motif.gif"></A>,
but RFIO takes account of the network characteristics
and the mode of use of the files to minimize overheads and maximize
throughput.
RFIO maintains portability by using only the BSD socket interface to TCP,
and thus operates over UltraNet, Ethernet, FDDI or other media.
RFIO transmits I/O calls
from client processes to remote RFIO daemons running on all SHIFT hosts.
<P>
RFIO is implemented with both C and FORTRAN interfaces. In C, the
system presents the same interface as local
Unix I/O calls:
<tt>rfio_open</tt>
opens a file like
<tt>open(2)</tt>,
<tt>rfio_read</tt>
reads data from a file like
<tt>read(2)</tt>
etc.
<P>
Most High Energy Physics programs are written in FORTRAN, and usually
interface their I/O via one or two intermediate library packages.
RFIO has been incorporated into these, so its usage is completely
transparent to the users of these programs.
RFIO performance is a key factor in SHIFT
and a major effort was made to reduce the operating
system overheads.
<P>
<H2><A NAME=SECTION0033400000000000000> Magnetic Tape Support</A></H2>
<P>
A portable Unix Tape Subsystem was designed to
satisfy all SHIFT's requirements in the area of cartridge tape access.
The subsystem runs on all SHIFT hosts to which tape devices are connected.
<P>
Unix systems usually offer a primitive tape interface
which is ill adapted to a multiuser environment. Four basic
functions are typically provided:
<tt>open(2), read(2), write(2), close(2)</tt>
Several
<b>ioctl(2)</b>
commands are also provided but there is no operator interface,
label processing, or interface to a tape management system.
The SHIFT Tape Subsystem
offers dynamic configuration of tape
units, reservation and allocation of the units, automatic label
checking, an operator interface, a status display and an interface
to the CERN/Rutherford Tape Management System.
It is written entirely as user code and requires no
modification of manufacturers' driver code.
It currently supports StorageTek's 4280 SCSI tape
drive (an IBM 3480 compatible), Exabyte 8200/8500 drives
and DAT (on HP only).
<P>
Tape file labelling is provided by a set of user callable routines.
In practice, most tape I/O is done by using
a tape staging utility, RTCOPY which hides details of these
routines from the user.
<H2><A NAME=SECTION0033500000000000000> Remote Tape Copy Utility, RTCOPY</A></H2>
<P>
To provide tape access for every SHIFT CPU and disk
server, a tape copy utility RTCOPY was developed
which allows tape access across the network.
Internally RTCOPY uses RFIO software to maximize the
data transfer speed and thus minimize the tape unit allocation time.
RTCOPY intelligently selects an appropriate tape server, by polling
all known tape servers to query the status of their tape unit(s).
RTCOPY supplies any
missing tape identification parameters by querying the Tape Management System
as needed. RTCOPY then initiates the tape copy, informs the user when the
operation is complete, and deals with error recovery.
<P>
<H2><A NAME=SECTION0033600000000000000> Network Queueing System</A></H2>
<P>
The Network Queueing System, NQS, is a distributed batch scheduler originally
 developed
for NASA and which is now in the public domain.
<P>
NQS is used in SHIFT for
job submission and scheduling across a network
of Unix batch workers. At CERN, the public domain version of NQS has
been ported to numerous workstation platforms and useful enhancements
added. These include limits on the number of jobs run for any user
at one time, an interactive global run
limit, the ability to move requests from one queue to another and the
ability to hold and release requests dynamically.  In addition
CERN has implemented in
NQS the ability to have the destination server chosen automatically, based
on relative work loads across the set of destination machines.
<P>
Users submit
jobs to a central pipe queue which in turn chooses a destination batch queue
or
<em>initiator</em>
on the least loaded machine that meets the jobs' resource requirements.
If all initiators are busy, jobs are held in the central pipe queue and only
released when one becomes free.
A script running above NQS holds or releases waiting jobs
with a priority based on their owner's past and current
usage of the SHIFT service.
<P>
An NQS command summary is given below.
<P>
<P>
<PRE>



  qcat      Display output files of NQS running requests.
  qcmplx    Display status of NQS queue complexes.

  qdel      Delete or signal NQS requests.
  qhold     Hold NQS requests.
  qjob      Display status of NQS networked queues.

  qlimit    Show supported batch limits and shell strategy.
  qrls      Release NQS requests

  qstat     Display status of NQS requests and queues.
  qsub      Submit an NQS batch request.

  qmgr      NQS queue manager program
</PRE><P><H2><A NAME=SECTION0033700000000000000> Accounting Facilities</A></H2>
<P>
Several utilities have been developed to provide resource consumption
reports and to monitor usage of the systems.
At weekly intervals, a SHIFT accounting report is returned to the
central CERN database. This report shows CPU usage on all SHIFT systems both by
individual user and by user group.
<P>
<HR> <A NAME=tex2html109 HREF=chapter2_4.html><IMG ALIGN=MIDDLE SRC="http://asis01.cern.ch/CN/CERNTEX/icons/next_motif.gif"></A> <A NAME=tex2html107 HREF=shiftuser.html><IMG ALIGN=MIDDLE SRC="http://asis01.cern.ch/CN/CERNTEX/icons/up_motif.gif"></A> <A NAME=tex2html101 HREF=chapter2_2.html><IMG ALIGN=MIDDLE SRC="http://asis01.cern.ch/CN/CERNTEX/icons/previous_motif.gif"></A> <A NAME=tex2html111 HREF=tableofcontents2_1.html><IMG ALIGN=MIDDLE SRC="http://asis01.cern.ch/CN/CERNTEX/icons/contents_motif.gif"></A> <BR>
<B> Next:</B> <A NAME=tex2html110 HREF=chapter2_4.html> The SHIFT Configuration </A>
<B>Up:</B> <A NAME=tex2html108 HREF=shiftuser.html>No Title</A>
<B> Previous:</B> <A NAME=tex2html102 HREF=chapter2_2.html> Introduction</A>
<HR> <P>
<HR>

</BODY>
<P><ADDRESS>
<I>goossens@ <BR>
Fri Mar  4 17:06:49 MET 1994</I>
</ADDRESS>