head     1.1;
access   ;
symbols  ;
locks    goossens:1.1; strict;
comment  @@;


1.1
date     94.03.14.14.50.34;  author goossens;  state Exp;
branches ;
next     ;


desc
@ Initial checkin
@



1.1
log
@Initial revision
@
text
@\Part{IntepDretation of the Errors on Parameters as given bv Minuit} 
 
It often happens that the solution of a minimization problem uCing MINUIT is itself straightforward, but the calculation or interpretation of the resulting parameter uncertainties is considerably more complicated. The purpose-of this note is to clarify the
› most commonly encountered difficulties in parameter error determination. These difficulties may arise in connection with any fitting program, but will be discussed here with MINUIT terminology for convenience of MINUIT users.
 
The most common causes of misinterpretation may be grouped into three categories:
 
1. Proper normalization of the user-supplied chi-square or likelihood function, and appropriate ERROR DEF.
 
2. Non-linearities in the problem formulation, leading to different errors being calculated by different techniques, such as MIGRAD, HESSE, and MINOS.
 
3. Multiparameter error definition and interpretation.
 
All these topics are discussed in some detail in Eadie et al.\cite{bib-EADIE},
which may be consulted for further details.

\section{Function normalization and ERROR DEF}

In order to provide for full generality in the user-defined function value, 
the user is allowed to define a normalization factor known internally as UP 
and defined by the MINUIT user on an ERROR DEF command card. 
The default value is one. 
The MINUIT error on a parameter is defined as the change of parameter which would 
produce a change of the function value equal to UP. 
This is the most general way to define the error, although in statistics it is 
more usual to define it in terms of the second derivative of the (chi-square) function 
--with respect to the parameter in question. 
In the simplest linear case (when the function is exactly parabolic at the minimum), 
the value \Lit{UP=1.0} corresponds to defining the error as the inverse of the second 
derivative at the minimum. 
The fact that MINUIT defines the error in terms of a function change does not mean 
that it always calculates such a function change. 
Indeed it sometimes (HESSE) calculates the second derivative matrix and 
inverts it, assuming a parabolic behaviour. 
This distinction is discussed in chapter 2 of this note.

The purpose of defining errors by function changes is threefold: 

\begin{enumerate}
\item to preserve its meaning in the non-parabolic case (see chapter 2), 
\item to allow generality when the user-defined function is not a chi-square 
or likelihood, but has some other origin 
\item to allow calculation not only of ``one-standarddeviation'' errors, 
but also two or more standard deviations, or more general ``confidence regions'', 
especially in the multiparameter case (see chapter 3).
\end{enumerate}

\subsection{Chi-square normalization}

If the user's function value $F$ is supposed to be a chisquare, it must of course be properly normalized. That is, the 'weights' must in fact correspond to the one-standard-deviation errors on th~e observations. The most ~eneral expression for the chi-square
› X is of the form (see Eadie et al, p.163):
 
x2 = Sum (x -y (a)) V (x -y (a)) i,; i i      ij j ~
 
where x is the vector of observation~, y(a) is the vector of fitted values (or theoretical expressions for them) containing the variable fit parameters a, and V is the inverse of the error matrix of the observations x, also known as the covariance matrix o
›f the observations.
Parameter Errors            F. James                       Page 4
 
Fortunately, in most real cases the observations x are statistically independent of each other (e.g., the contents of the bins of a histogram, or measurements of points on a trajectory), so the matrix V is diagonal only. The expression for X- then simplifi
›es to the more familiar form:
 
X~ = Sum
 
where eL is the inverse of the diagonal element of V, the square of the error on the corresponding observation x. In the case where the x are integer numbera of events in an unweighted histogram, for example, the e2 are ~ust equal to the x (or to the y, se
›e Eadie et al, pp.170-171).
 
The minimization of X~ above i~ sometimes called wei~hted east s~uares in which caqe the inverse quantities 1/eL are called the weights. Clearly this is simply a different word for the same thing, but in practice the use of these words sometimes means that
› the interpretation of e2 as variances or squared errors is not straightforward. The word weight often implies that only the relative weights are known ('point two is twice as important as point one') in which case there is apparently an unknown overall n
›ormalization factor. Unfortunately the parameter errors coming out of such a fit will be proportional to this factor, and the user must be aware of this in the formulation of his problem.
 
The e may also be functions of the fit parameters a (see Eadie et al, pp.170-171). Normally this results in somewhat slower convergence of the fit since it usually increases the nonlinearity of the fit. (In the simplest case it turns a linear problem into 
›a non-linear one.) However, the effect on the fitted parameter values and errors shQuld be small.
 
If the user's chi-square function is correctly normalized, he should use UP=1.0 (the default value) to get the usual onestandard-deviation errors for the parameters one by one. To get two-std.-dev. errors, use ERROR DEF 4.0, etc., since the chisquare depen
›dance on parameters is quadratic. For more general confidence regions involving more than one parameter, see below, ch. 3-
Parameter Error~            F. James                       Page 5
 
1.2 Likelihood normalization
 
If the user function is a negative log-likelihood function, it must again be correctly normalized, but the reasons and ensuing problems in this case are quite different from the chisquare case. The likelihood function takes the form (see Eadie et al, p. 15
›5):
 
F = -Sum ln f(x ,a)
 
where each x represents in general a vector of observations, the a are the free parameters of the fit, and the function f represents the hypothesis to be fitted. This function f must be normalized:
 
f(x ,a) dx dx ... dx = constant i      1 2       n
 
that is, the integral of f over all observation space x must be independent of the fit parameters a.
 
The consequence of not normalizing f properly is usually that the fit simply will not converge, some parameter~ running away to infinity. Strangely enough, the value of the normalization constant doec not affect the fitted parameter values or errors, as ca
›n be seen by the fact that the logarithm makes a multiplicative constant into an additive one, which simply shifts the whole log-likelihood curve and affects its value, but not the fitted parameter values or errors. In fact, the actual value of the likeli
›hood at the minimum is quite meaningless (unlike the chi-square value) and even depends on the units in which the observation space x is expressed. The meaningful quantity is the difference in log-likelihood between two points in parameter-space, which is
› dimensionless.
 
For likelihood fits, the value UP=0.5 corresponds to onestd.-dev. errors. Or, alternatively, F may be defined as -2~10g(1ikelihood), in which case differences in F have the same meaning as for chi-square and UP=1.0 is appropriate. The two different ways of
› introducing the factor of 2 are quite equivalent in MINUIT, and although mo~t people seem to use UP=0.5, it is perhaps more logical to put the 2 directly into FCN.
Parameter Errors            F. James                       Page 6
 
~_ Non-linearities: MIGRAD v~. HESSE vs. MINOS
 
In the theory Or statistics, one can show that in the asymptotic llmit, any of several method9 or determining parameter errors are equivalent and will give the same result. Let us for the moment call theae methods MIGRAD, HESSE, and MINOS (SIMPLEX is a spe
›cial ca9e). It turns out that the conditlons under which these methods yield exactly the same errors are either Or the following:
 
(a) The model to be ritted (y or f) is exactly a linear function Or the fit parameters a, or
 
(b) The amount Or observed data is infinite.
 
It may happen that (a) 19 satlsrled, ln whlch case you don't really need MINUIT, a smaller, simpler, and raster program would do, since a linear problem can be solved directly without iterations (~ee Eadie et al, p. 163-165), rOr example with CERN library 
›program LSQQR. Neverthele99, it may be convenient to use MINUIT slnce non-linear terms can then be added later ir desired, without ma~or changes to the method. Condition (b) is of course never 9ati9fied, although in practice it often happens that there i~
› enough data to make the problem 'almost linear', that i9~ there is 90 much data that the range Or parameters allowed by the data becomes very small, and any physical function behaves linearly over a small enough region.
 
The following sections explain the dirrerences between the various parameter errors given by MINUIT.
 
2.1 Errors ~rlnted bv MINUIT
 
The errora printed by MINUIT at any glven stage represent the best symmetric error estimates available at that ~tage, which may not be very good. For example, at the rirst entry to FCN, the u9er~9 step slzes are glven, and the9e may bear no resemblance at 
›all to proper parameter errors, although they are supposed to be order-or-magnltude estimates. After crude minimizers like SEEK or SIMPLEX, a revised error estimate may be given, but this too is only meant to be an order-or-magnltude estimate, and mu-~t c
›ertainly not be taken ~eriously as a physical result. Such numbers are mainly for the internal use Or MINUIT, which must arter all assume a step gize ror ruture minimizations and derivative calculations, and u~e9 these 'errors' as a rirst guess to be modi
›fied on the basis Or experience.
Parameter Errors            F. James                       Page 7
 
2.2 Errors after MIGRAD (or MINIMIZE)
 
The minimizing technique currently implemented in MIGRAD is a stable variation (the 'switching' method) of the DavidonFletcher-Powell algorithm, described, as are all MINUIT minimization algorithms, in the reprint 'Function Minimization' by F. James, avail
›able from the CERN Program Library or Computer Science Library as a supplement to the MINUIT long-write-up. This alsorithm converges to the correct error matrix as it converges to the function minimum. In practice, MIGRAD usually yields good estimates of 
›the error matrix, but it is not absolutely reliable for two reasons:
 
(a) Convergence to the minimum may occur 'too fast' for MIGRAD to have a good estimate of the error matrix. In the most flagrant of such cases, MIGRAD realizes this and automatically introduces an additional call to HESSE (described below), informing the u
›ser that the covariance matrix is being recalculated. Since, for n variable parameters, there are n(n+1)~2 elements in the error matrix, the nu~mber of FCN calls from MIGRAD must be large compared with n~ in order for the MIGRAD error matrix calculation t
›o be reliable.
 
(b) MIGRAD gathers information about the error matrix as it proceeds, based on function values calculated away from the minimum and assuming that the error matrix is nearly constant as a function of the parameters, as it would be if the problem were nearly
› linear. If the problem is highly non-linear, the error matrix will depend strongly on the parameters, MIGRAD will converge more slowly, and the resulting error matrix will at best represent some average over the last part of the trajectory in parameter-s
›pace traversed by MIGRAD.
 
If MIGRAD errors are wrong because of (a), HESSE should be commanded after MIGRAD and will give the correct errors. If MIGRAD errors are wrong because of (b), HESSE will help, but only in an academic sense, since in this case the error matrix is not the wh
›ole story and for proper error calculation MINOS must be used.
 
As a general rule, anyone seriously interested in the parameter errors should always put at least a HESSE command after each MIGRAD (or MINIMIZE) command.
Parameter Errora            F. James                       Page 8
 
2.~ Errors after HESSE
 
HESSE simply calculates the full second-derivative matrix by finite differences and inverts it. It therefore calculates the error matrix at the point where it happens to be when it is called. If the error matrix is not positive-definite, diagnostics are pr
›inted, and an attempt is made to form a positive-definite approxlmation. The error matrix must be positive-definite at the solution (minimum) for any real physical problem. It may well not be positive away from the minimum, but most algorithms including t
›he MIGRAD algorithm require a positive-definite 'working matrix'.
 
The error matrix produced by HESSE is used to calculate what MINUIT prints as the parameter errors, which therefore contain the effects due to parameter correlations. The extent of the two-by-two correlations can be seen from the correlation coefficients p
›rinted by ~INUIT, and the global correlation~ (see Eadie et al, p. 23) are also printed. All of these correlation coefficients must be less than one in absolute value. -If any of them are very close to one or minus one, this indicates an illposed problem 
›with more free parameters than can be determined by the model and the data.
 
2.4 Errors bv ~INOS
 
MINOS is designed to calculate the correct errors in all cases, especially when there are non-linearities as described above. The theory behind the method is described in Eadie et al, pp. 204-205 (where 'non-parabolic likelihood' should of course read 'non
›-parabolic log-likelihood', which is equivalent to 'nonparabolic chi-square').
 
MINOS actually follows the function out from the minimum to find where it crosses the function value (minimum + UP), instead of using the curvature at the minimum and assuming a parabolic shape. This method not only yields errors which may be different fro
›m those of HESSE, but in general also different positive and negative errors (asymmetric error interval). Indeed the most frequent result for most physical problems is that the (symmetric) HESSE error lies between the positive and negative errors of MINOS
›. The difference between these three numbers is one measure of the non-linearity of the problem (or rather of its formulation).
 
In practice, MINOS errors usually turn out to be close to, or somewhat larger than errors derived from the error matrix, although in cases of very bad behaviour (very little data or illposed model) anything can happen. In particular, it is often not
Parameter Errors            F. James                       Page g
 
true in MINOS that two-qtd.-dev. errors (UP=4) and threestd.-dev. errors (UP=9) are recpectively two and three times as big as one-std.-dev. errors, as is true by definition for errors derived from the error matrix (MIGRA~ or HESSE).
Parameter Errors            F. James                      Page 10
 
. Multi~arameter error~
 
In addition to the difficulties de~cribed above, a special class of problems arise in interpreting errors when there are more than one free parameters. These problems are quite separate from those described above and are really much simpler in principle, a
›lthough in practice confusion often arises.
 
~.1 The Error Matrix
 
The error matrix, also called the covariance matrix, is the inverse or the ~econd derivative matrix of the (log-likelihood or chi-square) function with respect to its free parameters, usually assumed to be evaluated at the best parameter values (the functi
›on minimum). The diagonal elements of the error matrix are the squares of the individual parameter errors, includin~ the erfects of correlationa with the other parameters.
 
The inverse of the error matrix, the second derivative matrix, has as diagonal elements the second partial derivatives with respect to one parameter at a time. These diagonal elements are not therefore coupled to any other parameters, but when the matrix i
›s inverted, the diagonal elements of the inverse contain contributions from all the elements of the second derivative matrix, which is 'where the correlations come from'.
 
Although a parameter may be either positively or negatively correlated with another, the effect of correlations is always to increase the errors on the other parameters in the sen~e that if a given free parameter suddenly became exactly known (fixed), that
› would always decrease (or at least not change) the errors on the other parameters. In order to see this effect quantitatively, the following procedure can be used to 'delete' one parameter from the error matrix, including its effects on the other paramet
›ers: (1) Invert the error matrix, (2) Remove the row and column of the inverse corresponding to the given parameter, and (3) Re-invert the resulting (smaller) matrix. This reduced error matrix will have its diagonal elements smaller or equal to the corres
›ponding elements in the orisinal error matrix, the difference representing the effect of knowing or not knowing the true value of the parameter that was removed at step two. This procedure is exactly that performed by MINUIT when a FIX command is executed
›. Note that it is not reversible, since information has been lost in the deletion. The MINUIT commands RESTORE and RELEASE therefore cause the error matrix to be considered lost and it must be recalculated entirely.
Parameter Errors            F. James                      Page 11
 
~.2 MINOS with several free Darameters
 
The MINOS algorithm is described in some detail in the MINUIT long-write-up and will not be repeated here, but we will add some supplementary 'geometrical interpretation' for the multidimensional case (which is the usual case -- in fact, early versions of 
›MINOS had a bug which prevented them from working in the one-parameter case because it had not occurred to the authors that anybody would use it for only one parameter!).
 
Let us consider that there are just two free parameters, and draw the contour line connecting all points where the function take~ on the value Fmin + UP. (The CONTOUR command will do this for you from MINUIT). For a linear problem, this contour line would 
›be an exact ellipse, the shape and orientation of which are described in Eadie et al, p.196 (fig. 9.4). For our problem let the contour be as in Fig. 1 below. If MINOS is requested to find the errors in parameter one (the x-axis), it will find the extreme
› contour points A and B, whose x-coordinates, relative to the x-coordinate at the minimum (X), will be respectively the negative and positive MINOS errors of parameter one.
Parameter Errors	F. James			Page 12
 
	I ooooooo
	oo	ooooo
	o I		ooo
							oo	I						B
						oo		,						oo I
					oo			,					oo
				o				'				oo
			oo					I			oo
		ooo						'		o				O
	oo							'	o
 
             I o								, o						param. 1
             I oo								' o
             A								I o
              o	,	o
 
o		 lo
oo	oo
 
oo           oo I
 
ooo     ooo ooooo
 
Fig. 1 - MINOS errorC for parameter 1
 
~.~ Probabilitv content of confidence ~e~ions
 
For an n-parameter problem MINOS performs minimizations in (n-1) dimensions in order to find the extreme points of the hypercontour of which a two-dimensional example is given in Fig. 1, and in this way takes account of all the correlations with the other 
›n-l parameters. However, the errors which it calculates are still only single-parameter errors, in the sense that each parameter error is a statement only about the value of that parameter. This is represented geometrically by saying that the confidence r
›egion expressed by the MINOS error in parameter one is the cross-hatched area of Fig. 2, extending to infinity at both the top and bottom of the figure.
Parameter Errors F. James	Page 13
 
////////////~/////// l ///////////////
//////////////////// l ///////////////
//////////////////// l ///////////////
//////////////////// I ///////////////
//////////////////// I ooooooo//////// ///////////////////oo///////ooooo/// //////////////////o/ o ////////////ooo ////////////////oo// I ///~///////////~ //////////////oo//// I /////////////oo ' ////////////oo////// I ///////////oo// I ///////////o///////
›/ I /////////oo//// I /////////oo///////// ' ///////oo////// I //////ooo///////////, //////o//////// I ////oo//////////////, /////o///////// I
 
//o/////////////////'///o///////////param. 1
I oo////////////////// I //o////////////
A////////////////////,/o/////////////
o/////////////////// ' /o/////////////
/o////////////////// ' o//////////////
//oo///////////////oo///////////////
////oo///////////oo/ I ///////////////
//////ooo/////ooo/// I ///////////////
/////////ooooo////// I ///////////////
//////////////////// I ///////////////
//////////////////// ' ///////////////
//////////////////// I ///////////////
//////////////////// I ///////////////
 
Fig. 2 - MINOS error confidence region for parameter 1
 
If UP is set to the appropriate one-std.-dev. value, then the precise meaning of the confidence region of Fig. 2 is: "The probability that the true value of parameter one lies between A and B ic 68.3%" (the probability of a Normally-distributed parameter l
›ying within one std.-dev. of its mean). That is, the probability content of the cross-hatched area in Fig. 2 i9 68.3S. No statement is made about the simultaneous values of the other parameter(s), since the cross-hatched area covers all values of the othe
›r parameter(s).
 
If it is desired to make simultaneouslv statements about the values of two or more parameters, the situation becomec cosiderably more complicated and the probabilities get much smaller. The first problem is that of choosing the shape of the confidence regi
›on, since it is no longer aimply an interval on an axis, but a hypervolume. The easiest shape to express i3 the
Parameter Errors	F. James		Page 14
 
hyperrectangle given by:		-
A < param 1 < B
C < param 2 < D
E < param 3 < F , etc.
 
This confidence region for our two-parameter example is the cross-hatched area in Fig. 3. 
However, there are two good reasons not to use such a shape:
(a) Some regions inside the hyperrectangle (namely the corners) have low likelihoods, 
lower than some regions just outside the rectangle, so the hyperrectangle is not 
the optimal shape (does not contain the most likely points).
(b) One does not know an easy way to calculate the probability content of these 
hyperrectangles (see Eadie et al, p.196-197, esp. fig. 9.5a).
 
param 2'
 
loooDooo
///////////////////oo///////ooooo///
//////////////////o/ I ////////////ooo
////////////////oo// I ///////////////B
//////////////oo//// I /////////////oo
////////////oo////// I ///////////oo~/,
///////////o//////// I /////////oo//// I
/////////oo///////// ' ///////oo////// t
//////ooo/////////// I //////o////// oo////////////// I /////o///////// '
 
//o/////////////////l///o/////////// param. 1
' oo////////////////// I //o////////////
A////////////////////l/o/////////////
o///////////////////, /o///////////// /o////////////////// ' o////////////// //oo///////////////oo/////////////// ////oo///////////oo/ I /////////////// //////ooo/////ooo/// ' /////////////// ooCoo ~
 
Fig. 3 - Rectangular confidence region for parameters 1 and 2
Parameter Errors            F. James                      Page 15
 
For these reasons one usually chooses reEions delimited by contours of equal likelihood (hyperellipsoidc in the linear case). For our two-parameter example, such a confidence region would be the cross-hatched region in Fig. 4, and the corresponding probabi
›lity statement is: "The probability that parameter one and parameter two simultaneously take on values within the one-std.-dev. likelihood contour is 39.3%.
 
param 2
 
I ooooooo oo///////ooooo o/ I ////////////ooo oo//l///////////////B oo//// I /////////////oo oo////// I ///////////oo o//////// I /////////oo oo///////// ' ///////oo ooo///////////, //////o oo////////////// ' /////o
 
o/////////////////' ///o	param.
oo////////////////// I //o
A////////////////////|/o
o///////////////////, /o
o////////////////// I o
oo///////////////oo
oo///////////oo
ooo/////ooo
ooooo
 
Fig. 4 - Optimal confidence region for parameters 1 and 2
 
The probability content of confidence regions like those shaded in Fig. 4 becomes very small as the number of parameters NPAR increases, for a given value of UP. Such probability contents are in fact the probabilities of exceeding the value UP for a chisqu
›are function of NPAR desrees of freedom, and can therefore be read off from tables of chisquare. Table 1 below gives the values of UP which yield hypercontours enclosing given probability contents for given number of parameters.
Parameter Error~            F. James                      Page 16
 
Table 1
Table of UP for multiDarameter confidence re~ions
 
I Confidence level (probability cont~ents desired Number of I in~ide hypercontour of "chi~ = chi min + UP") 
Parametersl   50%   1   70%   t   90~   1   95%   1   99
 
_ _ _ _ _ _ _ _ _ _ + _ _ _ _ _ _ _ _ _ + _ _ _ _ _ _ _ _ _ + _ _ _ _ _ _ _ _ + _ _ _ _ _ _ _ _ _ + _ _ _ _	+
 
1	i	0.46	1	1.07	j	2.70	j	3.84	i	 6.63
2	j	1.39	j	2.41	)	4.61	1	5.99	1	 9.21
3	1	2.37	1	3.67	1	6.25	1	7.82	1	11.36
4	1	3.36	1	4.88	j	7.78	j	9.49	1	13.28
5	i	4.35	)	6.06	j	9.24	j	11.07	j	15.09
6	1	5.35	1	7.23	j	10.65	1	12.59	j	16.81
7	j	6.35	j	8.38	j	12.02	j	14.07	j	18.49
8	i	7.34	1	9.52	1	13.36	j	15.51	j	20.09
9	1	8.34	1	10.66	j	14.68	1	16.92	j	21.67
10	i	9.34	1	11.78	j	15.99	j	18.31	j	23.21
11	'	10;34	1	12.88	1	17.29	1	19.68	1	24.71
I [If FCN is -log(likelihood) instead of chi-square, I all values of UP should be divided by 2.]
 

@
