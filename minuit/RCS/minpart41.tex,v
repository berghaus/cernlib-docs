head     1.1;
access   ;
symbols  ;
locks    goossens:1.1; strict;
comment  @@;


1.1
date     94.03.14.14.50.38;  author goossens;  state Exp;
branches ;
next     ;


desc
@ Initial checkin
@



1.1
log
@Initial revision
@
text
@|4|||||||||||||||'|||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||K|||||||||||||||||||||||||||||||||'||||||||||||||||||||||||||||||||||||||||||||=|/||||||||p||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||
›- 201 -
 
plicated and the probabilities get much smaller. The first problem is that of choosing the shape of the confidence region, since it is no longer simply an interval on an axis, but a hypervolume. The easiest shape to express is the hyperrectangle given by: 
›A < param 1 < B C < param 2 < D E < param 3 < F , etc.
 
param 21
 
|oooDooo
oo ~     ooooo~
o ~	o o o
 
/////~//~////oo/// ~	/////oo
/////~/////o o ////// /////////~o o //
 
oo ~            //oo ~
o o o ////////// ~      o ////////
 
1~o// ~ o///// ~	param. 1
 
~oo ~            oo ~
oo~/////////oo/ ////~//////~///
//////ooo///~ooo~ ~/////////////~ nn ~nn
 
Figure 7 - Rectangular confidence region for parameters 1 and 2
 
This confidence region for our two-parameter example is the crosshatched area in Figure 7. However, there are two good reasons not to use such a shape:
 
1. Some regions inside the hyperrectangle (namely the corners) have low likelihoods, lower than some regions iust outside the rectangle, so the hyperrectangle is not the optimal shape (does not contain the most likely points).
 
2. One does not know an easy way to calculate the probability content of these hyperrectangles (see Eadie et al, p.196-197, esp. fig. 9.5a).
 
For these reasons one usually chooses regions delimited by contours of equal likelihood (hyperellipsoids in the linear case). For our twoparameter example, such a confidence region would be the cross-hatched region in Figure 8, and the corresponding probab
›ility statement is: "The probability that parameter one and parameter two simultaneously take on values within the one-std.-dev. likelihood contour is 39.3%.
- 202 param 21
 
I ooooooo
o o /~/////o o o o o
o / ~      ////o o o oo// / ~ ////////B
	oo/ ~				oo
o o ~//// ////~/////~o o
o ~				o o
o o / ~			o o
ooo//// ~		/ //////o
oo/ ~		/////o
 
o// ~		o				param .
 
o o //// ~		o
A//////// ~	//// /o
o// ~	o
o// ~	o
o o ~	o o
o o ~	o o
o o o ~	o o o
o o o o o
 
Figure 8 - Optimal confidence region for parameters 1 and 2
 
The probability content of confidence regions like those shaded in Figure 8 becomes very small as the number of parameters NPAR increases, for a given value of UP. Such probability contents are in fact the probabilities of exceeding the value UP for a chis
›quare function of NPAR degrees of freedom, and can therefore be read off from tables of chisquare. Table 1 below gives the values of UP which yield hypercontours enclosing given probability contents for given number of parameters.
- 203 -
 
Table 1
Table of UP for multiearameter confidence reqions
 
Confidence level (probability contents desired
Number of inside hypercontour of ~X2 = X2m;n + UP~)
Parameters    50%   1   70~O   1   90%   I   95~O   1   99%
_ _ _ _ _ _ _ _ _ _+_ _ _ _ _ _ _ _ _+_ _ _ _ _ _ _ _ _., _ _ _ _ _ _ _ _ _+_ ___ _ _ _ _ _+_ _ _ _ _ _ _ _ _,
 
1	0.46	1.07	2.70	3.84	6.63
2	1.39	2.41	4.61	5.99	9.21
3	2.37	3.67	6.25	7.82	11.36
4	3.36	4.88	7.78	9.49	13.28
5	4.35	6.06	9.24	11.07	15.09
6	5.35	7.23	10.65	12.59	16.81
7	6.35	8.38	12.02	14.07	18.49
8	7.34	9.52	13.36	15.51	20.09
9	8.34	10.66	14.68	16.92	21.67
10	9.34	11.78	15.99	18.31	23.21
11	10.34	12.88	17.29	19.68	24.71
[If FCN is -log(likelihood) instead of chisquare, all values of UP should be divided by 2.]
- 204 -
 
2. TESTING OF HYPOTHESES.
 
In this chapter, we consider choosing between two or more well-defined hypotheses, for example measuring the parity of an elementary particle, which can only be positive or negative.
 
2.1 THE TEST STATISTIC AND OTHER DEFINITIONS.
 
In order to discuss the theory of hypothesis testing, a few basic concepts need to be defined, with the corresponding notation. The notation, although rather arbitrary, is fairly well standardized throughout the statistical literature, and we adopt here th
›e most commonly used symbols.
 
The two hypotheses under consideration may have free parameters whose values must also be estimated (composite hypotheses), but we assume for the moment that they are completely defined (simple hypotheses). The simplest or most important of the two hypothe
›ses is denoted Ho and called the null hyPothesis, probably because it often corresponds to "no effect" or "zero dependence". The other hypothesis, H1, is called the alternative hyPothesis. Our aim is to choose between the two hypotheses, based on some obs
›ervations, and especially to be able to express the significance of those observations in distinguishing between the two hypotheses.
 
We denote the space of all observations by W. This includes all possible outcomes of our experiment. We will want to divide this space into two regions: the critical reqion, ~ is the set of all observations for which we would reject Ho, and the acceptance 
›reqion, W-~, for which we would accept Ho. This will be done with the help of a function X(W) called the test statistic. The study of hypothesis-testing is thus reduced to the study of the properties of different test statistics and the resulting regions 
›of rejection and acceptance.
 
We can now define tlle important quantities in terms of probabilities of obtaining different values of the test statistic:
 
1. The level of siqnificance of the test, a, is the probability of X falling in the critical region ~ when Ho is in fact true.
 
P(X~|Ho) = a
 
2. The power of the test, l-n, is the probability of X falling in the critical region when H1 is true:
 
P(X~(J| H1 ) = 1
 
or, alternatively:
 
P(X~(W-~) I H1 ) =
- 205 -
 
3. The error of the first kind, or loss, occurs when Ho is rejected even though it is true. The probability for this happening is clearly a.
 
4. The error of the second kind, or contamination, occurs when Ho is accepted even though the alternative H1 is true. The probability for tllis is from the above definitions 3.
 
A good statistical procedure for testing hypotheses will therefore be one which chooses tlle function X and the region ~ so as to minimize both a and ~.
 
2.1.1 ExamPle: SeParation of two tyPes of events.
 
As an example let us suppose we are studying elastic proton-proton scattering, and we have a sample of events which contains both true elastic events and those witll additional n| production. On the basis of the measured momenta of the protons alone, we ha
›ve to decide, for each event, wllether an unseen n| was also produced, in order to obtain finally the biggest and purest possible sample of elastic events. For each event, the two hypotheses are as follows:
 
Ho: p~p ~ p~p
 
H1: P-'P ~ p+p~n|
 
We could choose as test statistic the missing mass for the event, a function of the measured momenta which, if the momenta were measured exactly, would be equal to the n| mass for unwanted events and zero for elastic evellts. Because of measurement errors,
› the expected distributions of missing mass under the two hypotheses will have some width, and in practice may appear as in Figure 9. It is clearly not possible to choose w so as to make both a and ~ zero at the same time, although either one could be mad
›e arbitrarily small at the expense of increasing the other. Physically, this means that if we set the acceptance level so as to lose very few true elastic events, we will also have to accept a large number of background events; if, on the other hand, we r
›equire a very pure sample of elastic events, we will have to settle for a big loss of number of events. Some compromise is necessary, based on the physics to be done. In order to reduce both a and ~ simultaneously, we would have to find a better test stat
›istic. For example, if it is known that the n| are usually very fast, it may be better to use missing energy or missing momentum to discriminate between the two types of events. The choice of test will be discussed in the next subsection.
- 206 -
 
p (M) p ( M / Ho)
 
;
	(M/Hl )
M~o	 M
 
M c
 
Figure 9. Test statistic distributions for Ho and H1.
 
2.2 CHOOSING A TEST.
 
The properties of a test between two simple hypotheses can be seen from the diagram of Figure 10, which SllOWS ~ as a function of a for four different tests. Such a curve for any admissible test must lie below the diagonal indicated, since for any point ab
›ove the diagonal the probability of accepting the wrong hypotllesis would be greater than the probability of accepting the right one, and surely it must be possible to do better than that.
- 207 -
 
NON - PERM ISSI BLE
REG10 N
 
O ~ ~
 
Figure 10. Basic properties of four different tests.
 
The best test will correspond to the lowest (a,~) curve, since that will have the lowest value of ~ for a given value of a. Thus test one is always worse than the other tllree, test two is sometimes better than test three (namely if we are interested in sm
›all values of ~ rather than small values of a), and test four is always better than the other three.
 
If the test is being applied in order to select data for later analysis, additional criteria may be important, namely that the selection procedure not introduce a bias in the distributions of interest for the data selected, or at least that the bias so int
›roduced be calculable.
 
2.3 THE NEYMAN-PEARSON TEST.
 
In the case of completely defined simple hypotheses, there is one test which can be shown to be always the best, in the sense of giving always the smallest ~ for a given a. This test, the Neyman-Pearson test, may be very complex computationally, especially
› when the space of observables is many-dimensional, but can always be defined in principle.
- 208 -
 
For a given value of the significance level a, the most powerful test will be that one which has the best critical reqion in the space of the observations. That is, among all those regions (J(a) which satisfy
 
f(X|Ho)dX = a
~ ( a )
 
we wish to find that region which minimizes ~, or maximizes the power
 
f(X|H1)dX
( a )
 
f(X|H1)
= f	f(X|Ho)dX
c,~ f (X|Ho)
 
But the last expression above is just the expectation of the likelihood ratio for hypothesis one divided by hypothesis zero, assuming hypothesis zero to be true. This means that the best critical region is such that this likelihood ratio is larger for all 
›points inside w than outside w, with the constraint that the "size" of the region is a.
 
In the general case, the observable space of X is many-dimensional and the Neyman-Pearson test consists of finding the hypercontour of constant likelihood ratio which divides this space into two parts such that the part corresponding to larger values of th
›e likelihood ratio has an integrated probability (test size) of a. This can be a very lengthy calculation. The usual simplification is to consider not the whole observable space X, but a one-dimensional test statistic t(X) as introduced earlier. Then the 
›test is no longer Neyman-Pearson and not necessarily optimal, depending on how good a choice of test statistic was made.
 
2.4 COMPOSITE HYPOTHESES.
 
In practice, real experiments often give rise to situations where both parameter estimation and hypothesis testing must be performed simultaneously. That is, the hypotheses to be tested are not completely defined in advance, but have free parameters whose 
›values must be estimated. Such a hypothesis is called a composite hypothesis. The mathematical theory of composite hypothesis testing is not as well developed as that of simple hypothesis testing. The general techniques which can be found are valid only a
›symptotically (and tlle asymptotic limit may be very high!) or for certain related "families" of hypotlleses. In many real cases, the only way to obtain realistic confidence levels is by resorting to Monte Carlo simulation.
- 210 -
 
A famous example of this problem is the attempt to determine whether a peak in a distribution of effective mass is a single peak or a split peak. In several examples it has been crucial to establish tlle significance of tlle evidence for split peaks becaus
›e simple peaks would correspond to elementary particle states easily accomodated by the existing quark theory whereas split peaks would llave implied resonances with exotic properties requiring a quite different approach to the whole theory. Tlle importan
›ce of the problem triggered extensive statistical studies which revealed a marked tendency for data generated with a simple-peak hypotllesis to fit the split-peak hypothesis better than the simple-peak model. This bias in the testing procedure can only be
› evaluated by Monte Carlo simulation, drawing samples from a known hypothesis and fitting them in the same way as tlle experimental data.
 
3. GOODNESS-OF-FIT.
 
In this chapter we consider the significance with which we can accept or reject a hypothesis, without specifying any alternative hypotheses. As for hypothesis testing, we will have a critical region ~ such that for all data X~(o we will reject tlle null hy
›potllesis. As before, we can find the probability a of rejecting Ho when it is true:
 
J' f(X|Ho)dX = a
~(a)
 
However we can no longer evaluate the probability of accepting Ho when it is false, since this would depend on the alternative hypothesis which is not specified. We have therefore a measure of the significance of evidence against Ho (if the fit is bad) but
› the significance of the evidence in favor of Ho coming from a good fit cannot be quantified.
 
3.1 CHOOSING 9 GOODNESS-OF-FIT TEST
 
Since there is no alternative hypothesis, we cannot calculate ~ and cannot know the power of a test. This in principle deprives us of any means of comparing goodness-of-fit tests since a test is better than another only if it is more powerful at rejecting 
›unwanted hypotheses. Since the unwanted hypothesis is not specified, it appears that we cannot have a realistic basis for choosing a test. Indeed we can expect some tests to be sensitive to certain kinds of deviations from the null llypothesis, and other 
›tests to be sensitive to others. With this in mind, there are at least two approaches which will help in choosing a test.
- 211 -
 
The first approach is ratller intuitive, but can be made more rigorous witll some use of statistical information theory (which we do not discuss explicitly in these notes). The idea is to make sure somehow that the test makes use of all the information rel
›ative to the hypothesis being tested, and that it does not llave any arbitrary procedures or parameters wllich would affect tlle value of a independently of the data and the hypothesis. We will see examples of how this is used below.
 
The second approach is to invoke a class of alternative hypotheses which allow us to estimate the power without being too specific about the alternatives. For example one can define the local ~ ~ of a test as its power against infinitesimal deviations from
› the null hypothesis. Still this is not as general as it may seem since even infinitesimal deviations may take different forms, but it is usually possible in this way to get a good comparison of tests under rather general conditions.
 
3.2 DISTRIBUTION-FREE TESTS.
 
The calculation of the confidence level for a given test involves an integration of a probability density function over a region which may be many-dimensional. Although such numerical calculations may not be too difficult when performed on modern computers
›, it has traditionally been necessary, and even today is still desirable, to avoid this calculation by means of a distribution-free test. Such a test involves a test statistic t(X) whose distribution is known (under the null hypotllesis) independent of th
›e distribution of the X. Many distribution-free tests have been found, and the appropriate distributions of their test statistics are either known analytically or, more frequently, tabulated so that the user can simply calculate t and read the correspondi
›ng significance level from a table.4
 
A well-known example of a distribution-free test is the chisquare test of goodness-of-fit of a probability density g(x) to density estimated experimentally in the form of a histogram. If the number of events observed in the ith histogram bin is n;, the val
›ue of x in the middle of the bin is x;, and tlle density g(x) is normalized to the total number of events observed, then tlle test statistic for goodness-of-fit
i s :
 
(g(x;)-nj)2
 
t i        ni
 
4 Traditionally all tests were distribution-free, so the term was not even used, it being assumed. Very recently we see a new kind of test being used (see 3.5), in wllich the confidence level must be recomputed for each case, something that would not have 
›been thinkable without modern computers.
- 214 -
 
Notice that this test involves no arbitrary binning, and is still very easy to calculate, although for very large data samples the ordering of the data may be longer than the histogramming required for the chisquare test.
 
3.3.3 The Smirnov-Cramer-Von Mises test.
 
This test is very similar to the Kolmogorov test described above, except that the measure of the distance between two cumulative distributions is taken to be the integrated squared distance instead of the maximum:
 
~oo
W = N ~ lS(x)-F(x)12 f(x) dx
[ S ( x ) - F ( x ) ] 2 d F ( x )
 
1
 
The corresponding formula for comparing two experimental distributions is somewhat more complicated and is given in Eadie et al, page 269, as are formulas and tables for determining the significance level a.
 
The computational complexity of this test is clearly somewhat greater than for the Kolmogorov test, wllich probably explains why it is less popular. However it has the distinct advantage that the test is exactly distribution-free for all values of N, altho
›ugh the significance level is independent of N only for 'large' N (in fact N>3 is enough!). It is also more appealing (and probably more powerful against most alternatives) because it is really a function of all the data and not iust the maximum distance.
 
Because it is free of binning, is sensitive to all the data, and is exactly distribution-free, the Smirnov-Cramer-Von Mises test is generally considered the most powerful goodness-of-fit test for one-dimensional data.
 
3.4 COMPARING MULTIDIMENSIONAL DISTRIBUTIONS.
 
When the data are more than one-dimensional, goodness-of-fit testing becomes considerably more difficult.
 
3.4.1 Chisquare in d dimensions.
 
In principle, the chisquare test for goodness-of-fit is dimension-free, in the sense that it can be defined independently of the dimensionality of the data. One merely compares the expected number of events in each data class (bin) with the actual number. 
›The dimensionality of the bin does not enter into tlle theory.
- 215 -
 
In practice, however, multidimensional bins cause not only computational problems (when the boundary of the space is curved) but the number of events required to have a minimum number per bin becomes enormous, unless the number of bins is reduced to a very
› small number per dimension. The reason is that the number of bins increases exponentially with dimensionality. It is therefore clear that for multidimensional data we should prefer a test which does not require binning.
 
3.4.2 Kolmoqorov-tyPe tests in d dimensions.
 
It is very appealing to try to extend tests based on order statistics to higher dimensionalities, since these tests (Kolmogorov, Smirnov-CramerVon Mises) use the data points as measured without binning. Such attemps llowever meet with several difficulties,
› both practical and theoretical, and such tests have not to my knowledge yet been used with success. The first difficulty is with the order statistics themselves, which lose some of their nice properties in higher dimensions, although they can still be de
›fined in a straightforward way:
 
S(X1,X2,...Xn) = number of points p such that
P1 <X1, Pz<X2, | | | Pn<Xn
 
For example, they now depend on the orientation of the axes in the d-dimensional space. Another difficulty is the computational complexity, since the definition of F now requires multidimensional integration, and other multidimensional difficulties arise i
›n defining the test statistic. Perhaps the most important barrier is that straightforward extension of both the tests given above are no longer distribution-free in many dimensions, so that one has no easy way to determine a.
 
If we are willing to give up distribution-free testing, the more recent permutation metllods given in the next section are promising for both one- and many-dimensions, and for both large and small sample sizes.
 
3.5 PERMUTATION TESTS FOR COMPARING TWO POINT SETS.
 
We wish to test the hypothesis that two sets of points are random samples from the same underlying distribution, where we do not know the underlying distribution. We wish to find a test valid for one or moredimensional points, and the test should not invol
›ve binning. Two things are needed:
 
1. A test statistic a which will be a measure of the distance between the point sets.
 
2. A way to measure the significance of the value of a for the two point sets, giving the significance level a.
- 217 -
 
If a good distance measure a iS available for the physical problem at hand, the permutation technique gives a good way to evaluate its significance. The method is completely non-parametric, does not involve any attempt to actually estimate the point densit
›ies in either set, and is always exactly valid for the data sample at hand, no matter how large or small it is. On tlle other hand, the computation required is large by traditional standards, since one is in fact recalculating a each time instead of using
› a distribution-free test and a standard table. By modern computing standards however, this is a small price to pay for the advantages gained.
 
4. CONCLUSIONS.
 
We have seen how to define and evaluate the statistical significance of experimental data in the three different contexts of parameter estimation, hypothesis testing, and goodness-of-fit testing. In a logical development of tlle subject it is easy to keep 
›these different contexts distinct and avoid confusion, but in solving real problems it may not be so clear exactly what question is being asked. This is partly because one is often asking several questions at once, partly because the different techniques 
›are, after all, related, and partly because the same functions of the data are used -- in different ways -- to answer different questions. For this reason we shall conclude these lectures by pointing out the relationships, both similarities and difference
›s, between the various problems and methods of solution.
 
4.1 CHISQUARE.
 
The ubiquitous chisquare function of Pearson is the source of much confusion because it can arise in many contexts. It is a measure of the distance between a set of observations and a model, namely the sum of squares of deviations of the observations from 
›the model, each deviation normalized by its standard deviation. The model may contain unknown parameters, in whicll case the chisquare function may be used to estimate these parameters and their uncertainties. In this case, the chisquare function is consi
›dered as a function of these free parameters (although it is also of course a function of the data) and minimized with respect to these parameters. Parameter uncertainties are estimated by finding the change in parameter value required to produce a given 
›chanqe in chisquare, as described in these lectures. The actual value of chisquare is not used in this case, since it is assumed that the model is correct.
 
The same function can however be used to test goodness-of-fit. In this case there are no free parameters and chisquare is considered as a function of the data. Now only the value of cllisquare is used, and compared with a table of values to find the confid
›ence level a. To confuse things further, the table is also referred to as a chisquare table, and it gives the integral of a function also called the chisquare function, integrated from the value obtained experimentally to infinity, thereby
- 218 -
 
giving the probability, under the null hypotllesis, of obtaining a value of chisquare greater than that actually found.
 
4.2 LIKELIHOOD.
 
The likelihood function can also be used in different ways, to do parameter estimation or hypothesis testing. It cannot however be used effectively for goodness-of-fit testing, essentially because the actual value of the likelihood function has no statisti
›cal interpretation. Only differences in log-likelihood are meaningful. We have seen their interpretation in parameter and interval estimation. The interpretation in hypotllesis-testing is really the same, since here we are concerned with tlle likelihood r
›atio between two hypotheses, and the logarithm of this ratio is of course the difference in the logarithms of the two likelihood values corresponding to the two hypotheses.
 
Therefore, using tlle likelihood function to determine the significance of the statement that the parameter ~=~O_+a is exactly equivalent to using the likelihood ratio to test hypothesis Ho ~ o against
H ~	O+a.
- 219 -
 
REFERENCES
 
Clopper, C.J. and Pearson, E.S., Biometrika 2 (1934) 404
 
Eadie,W.T., Drijard,D., James,F.E., Roos,M., and Sadoulet,B. Statistical Methods in ExPerimental Physics. North-Holland, Amsterdam (1971)
 
Friedman, J.H., Data AnalYsis Techniques for Hiqh Enerqy Particle Phvsics, Lectures presented at tlle CERN School of Computing, Godoysund, Norway, August 11-24, 1974, in CERN Yellow Report 74-23.
 
James, F., and Roos, M., MINUIT - A system for function minimization and analysis of the parameter errors and correlations. Comp. Phys. Comm., 10 (1975) 343-367.
 
James, F. and Roos, M., Errors on ratios of small numbers of events, Nuclear Physics B172 (1980) 475-480
 
James, F., Interpretation of the shape of the likelihood function around its minimum, Comp. Phys. Comm., 20 (1980) 29-35
- 220 -
 
INTRODUCTION TO BIT SLICES AND MICROPROGRAMMING
 
Andries van Dam
 
Brown University, Providence, Rhode-Island, USA
 
Abstract
 
Bit-slice logic blocks are fourth-generation LSI components which are natural extensions of traditional multiplexers, registers, decoders, counters, ALUs, etc. Their functionality is controlled by microprogramming, typically to implement CPUs and periphera
›l controllers where both speed and easy programmability are required for flexibility, ease of implementation and debugging, etc. Processors built from bit-slice logic give the designer an alternative for approaching the programmability of traditional fixe
›dinstruction-set microprocessors with a speed closer to that of hardwired "random" logic.
 
Introduction
 
The purpose of this set of annotated lecture transparencies is to give a brief introduction to the use of bit-slice logic in microprogrammed engines (CPUs) and controllers. A basic understanding of the goals of the technology and its potential will allow o
›ne to read the literature with some idea of what the important issues and design parameters might be. Bit slices will be placed in the spectrum of hardware/software building blocks, and their basic types and uses will be briefly illustrated. Since slices 
›are controlled typically by microprograms, an elementary review of that subject will also be given, especially to stress the crucial point that working with bit slices requires a proper (and integrated) understanding of hardware, firmware and software, as
› well as the use of proper tools and methodologies for each of these levels of design.
 
The reader is referred to Glenford J. Myers' excellent brand-new book Digital System Design with LSI Bit-Slice Logic (Wiley-Interscience, 1980) for a full treatment, to Prof. V. Zacharov's lecture notes on technology in these Proceedings for a review of th
›e generations of digital building blocks, and to Dr. C. Halatsis' lecture notes on software for microprocessors, also in these Proceedings, for a discussion of tools and methodology for working with bit slices. As a relevant example, the MICE PDP-ll fixed
›-point engine used for online data filtering was built at CERN using bit-slice technology, and is described in References 1 and 2.
- 221 -
 
Review of Conventional CPU E~|i~
 
Figure 1 is a high-level block diagram of a conventional CPU, showing both processing (data transformation in the APU) and control (affecting data transfers between components, registers, and activations of functional units such as the ALU, shifter, etc.).
› Figure 2 shows a typical MOS fixed-instruction-set microprocessor implementation of such a CPU, with a CPU chip for most CPU functions (including the instruction cycle-fetch, increment, execute, ALU functions, I/O functions for both memories and devices 
›and control of other chips), a bus control chip for arbitrating and controlling the three standard busses (address, data, and control), the clock chip to provide timing pulses/waveforms to drive all other chips, and an Interrupt Control Unit chip to provi
›de a priority interrupt mechanism. Note that memory and device controller chips are handled as much as possible in the same way, each connected to the three standard busses.
 
Bit Slices
 
With current MOS (V)LSI technology, it is possible to pack all of the CPU functionality for an 8-bit or even a 16-bit processor in one or a few chips, where the limitation is one of pin-out - 64 pins is a reasonable upper limit with today's state of the ar
›t. Given the traditional speed/power/packing density tradeoffs between MOS and bipolar, the obvious question is whether a 64-pin bipolar microprocessor for an 8-bit or 16-bit processor is feasible. The answer, not surprisingly, is no, because of packing d
›ensity. What then is the right functionality to assign to bipolar chips? Figure 3 shows a simple functional decomposition of a CPU, where one or more function rows are represented by each chip of a traditional MOS microprocessor. Bipolar bit slices, becau
›se of their lesser packing density, cannot even take care of an entire 16-bit row's functionality, so they only do a 4-bit slice worth (shown in the vertical ruling), but in such a way that slices can be chained together, four for example, to give 16-bit 
›functionality. (Some families may provide only 2-bit slices and others may include 8-bit building blocks, particularly for memories.) This technique should be viewed simply as a natural extension of using well-known (4-bit) digital building blocks for mak
›ing arbitrary-size (de)multiplexors, registers, counters, shifters, (R)ALUs (register file plus ALU), etc.
 
On the spectrum of hardware/software building blocks shown in Figure 4, it is clear that bit slices are bipolar LSI hardware chips providing a level of functionality between that of MSI random-logic components and programmable MOS microprocessor CPU chips,
› but much closer to MSI components! It is, in fact, a misnomer to speak of bit-slice
_
 
- 222 -
 
microprocessor chips since they can only be used as building
blocks in a "microprocessor" CPU design, in the same way in
which standard components such as RALUs are. Bit slices are
hardware, not software, but require external microprogrammed
control to regulate their functionality, as discussed below.
 
To put it another way, bipolar LSI technology, coupled with microprogramming as the control technique, have resulted in a new medium for digital design which gives an excellent compromise between the speed of random (hardwired) bipolar logic and the flexib
›ility of the programmable microprocessor CPU chip. Cycle times for bit-slice designs vary from roughly 100 to 250 nsec (compared to 15-100 for the fastest ECL SSI designs). While not as fast as discrete hardware, bit-slice processors are also not as easy 
›to program as conventional microprocessors because, as we shall see, microprogramming involves far more intimate knowledge of hardware than does even assembly-language programming for a conventional microprocessor. Nonetheless, the speed advantage of a fa
›ctor of typically 2-5 over such MOS microprocessors may often allow the use of bit slices where previously a (non-programmable) hardwired design would have been required. Furthermore, bit slices, by their very nature, allow processor width appropriate to 
›the problem, thereby avoiding the time multiplexing of busses of conventional microprocessors whose widths are often too small.
 
In Figure 5 we see a high-level diagram of a conventional 4-bit ALU slice making a 16-bit ALU. Each ALU slice communicates with three data busses, two inputs (A and B) and one output (O); the 16-bit operands are split so that the high-order bits (0-3) are 
›handled by the high-order slice, 4-7 by the second slice, etc. All four slices operate in parallel under the control of the 7-bit shared opcode bus which has them all execute the same ALU function. (Only the control of carry-in and carry-out may differ in
› first and last stages.) Status is propagated in the normal fashion between stages, resulting in 16-bit status at the output of the high-order stage. Also, for ALU slices, look-ahead carry logic is typically available.
 
In addition to ~h_ slices (with or without built-in register file), manufacturer families typically support a sequencer (or microprogram controller) slice for controlling all other slices (and additional MSI/SSI logic such as multiplexors and registers), e
›xternal register file slice, priority interrupt controller slice, DMA controller slice, memory interface slice, etc., all of which are compatible with conventional logic and memories in the same technology. The most important (R)ALU and sequencer slices a
›re discussed in more detail below.
 
A typical design for a bit-slice-based processor is shown in Figure 6. Note the 16-bit arithmetic section made
_
 
from 4 RALU slices, whose status bits are fed, along with other status bits, to the control section. A 12-bit, 3slice microsequencer fetches microinstruction control words from the microprogram control store and stores each for execution in a pipeline regi
›ster, in effect a microinstruction register. Bits in the control word are used to condition ("program") each of the slices, busses, and other (discrete) logic in the processor to carry out the selected functionality for the duration of the microcycle. Aga
›in, this microprogrammed control will be explained in more detail below.
 
A slightly more detailed design is shown in Figure 7, which is based on the Motorola 10800 family of slices. A 16-bit ALU is made from 4 ALU slices plus look ahead carry chips, taking operands from an external register file made from two 8-bit dual-ported 
›slices. An 8-bit, 2-slice sequencer receives the low-order bits from the IB and OB busses and reads from 256 locations of control store. Some of the individual fields of the microinstruction are shown in the pipeline register. Finally, a 16-bit memory int
›erface, using 4 slices, controls an external memory and provides addressing via effective address arithmetic using address data from the register file.
 
A data filtering engine designed at CERN DD which can be microprogrammed directly or programmed in PDP 11 assembler or Fortran via an emulator for the PDP 11 ( i . e., a microprogrammed interpreter) is shown in Figure 8. At this level of detail it is very 
›similar to the previous design, using a 12-bit sequencer and a 118 bit-wide microinstruction/control store format. Note again the division of the microinstruction into fields controlling both the slices and the additional logic required to glue the slices
› together (such as the target instruction decoding ROMs).
 
This glue is added to the basic bit slices both to provide simple functionality which they don't have (e.g. additional 4-bit multiplexor "slices", registers, counters, memories, etc. preexisting in the logic family) and to bypass functionality of a slice w
›hich is too slow to be utilized. Thus the auxiliary address control is sometimes used to bypass the 10803 memory interface address calculation when to use it would require an extra cycle. Similarly, the target instruction decoding ROM gives singlecycle ma
›pping of a PDP 11 target instruction address mode to the microsubroutine to do the corresponding address calculation, bypassing the multi-microcycle functionality of the 10801 sequencer to do the next microaddress calculation.
 
This points out that the designer need not, indeed cannot use all the functionality of a slice, and must supplement it with standard components to provide missing functionality or to bypass functionality that is too slow. On
- 224 -
 
the average, however, most functions can be executed by the slices if one studies the data sheets very carefully to see how the many data and control functions can be marshalled. This is no small task, given the often-cryptic and idiosyncratic descriptions
› on the sheets; what's worse, slices are notoriously unstructured and asymmetrical (unorthogonal) only certain combinations of data flows and internal operations work, for reasons typically left unspecified. Experimentation may be required to find out exa
›ctly what a slice is capable of!
 
Review ~f Microprogramming
 
Each bit slice has typically one or more (4-bit) operand data pins and from 5 to 20 control pins which condition its data and control paths/options/functions, much in the way that a classical ALU has two 4-bit source operands, a 4-bit destination operand, 
›and a 5-bit opcode to select among 16 arithmetic and 16 logic functions to be applied to the sources (and destination). Rather than getting lost in the details of controlling each individual pin on each slice, let's step back a moment and look at the prob
›lem of control in digital computers.
 
Figure 9 shows a finite-state graph representation of the fetch-increment-execute instruction cycle, with much detail hidden by the use of macro states. A transition is made from State1 to State~ as a function of input, either a clock pulse or a clocked/s~
›robed data/control input. States can be implemented via registers, transitions by changing the contents of those registers. Any finite-state graph can be implemented as a collection of flip-flops/registers and combinatorial logic controlling the inputs to
› the registers (using a synthesis technique not needed here).
 
What is important to us is the notion that all actions in a computer can be divided into source -> destination data flows (register transfers) and activations of processing (data transformation) functions such as ALU or shifter units. The latter process ca
›n be symbolized as a +V -> processing unit "transfer" (Figure 10a). As with the state graph formalism, the trick is to be complete in the enumeration: itemize all states and transitions in and out of them; itemize all registers, processing units, data pat
›hs and control paths, and show which is active under which conditions (Figure 10b). In Figure 10c we show a method of synthesizing control of an individual source -> destination register transfer. The control network enabling both the data flow and its st
›robing into the destination flip-flop is simply an enumeration of the form: "if it's an add instruction (corresponding to a certain bit pattern in the opcode bits of the instruction register io~ il, ..., i7), and it's clock pulse 2 and major phase 3 or if
› it's the subtract instruction...".
 
|6||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||*|||-||||||||||||||||||||||||||||||||||v|m|d|[|R|I|||||||||||||||||||||||||||||||||||||||||||||||||||||||||||
|||||||||||||||||||||||||||}||||||||||||||||v|m|d|[|R|I|||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||lv|m|d|[|R|I|||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||l|||w
›|||x|||||||||||||||||||||||||||||||A|||K|||Lv|m|d|[|R|I|||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||L|||R|||||||||||||||||||||||||||||||||||||||||||zv|m|d|[|R|I|||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||z|||
›||||%|||*|||+|||-|||.|||1|||2|||7|||8|||<||||v|m|d|[|R|I||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||v|m|d|[|R|I||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||
›|||||||||||||||||||||||||||||||||!|||"|||(|||yv|m|d|[|R|I|||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||y||||||||||||||||||||||$.||$1||$2||$@@||$A||$E||%|v|m|d|[|R|I||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||%||
›|%|||%|||%|||%|||%|||&|||&|||&|||& ||&!||&'||'Pv|m|d|[|R|I||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||'P||'Z||'[||'`||*|||*|||*|||*|||4W||4_||4b||4q||4rv|m|d|[|R|I||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||4r
›||4v||9|||9|||:|||:|||:|||:|||A|||A|||A|||B|||B|v|m|d|[|R|I||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||B|||B|||B||B|||F|||F|||F|||F|||F|||G||G~||G|||G|v|m|d|[|R|I||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||G|
›||G|||G|||G|||G|||G|||J|||J|||J|||J|||J|||J|||J|v|m|d|[|R|I||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||J|||J|||J|||J|||P|||P|||P ||P%||P&||P)||P*||P3||P4v|m|d|[|R|I||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||P
›4||P7||P8||P=||P>||PB||X|||X|||[ ||[&||['||[)||[*v|m|d|[|R|I||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||[*||[3||\j||\o||\p||\r||\s||\}||^4||^>||bN||bY||bZv|m|d|[|R|I||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||
›bZ||ba||bb||bd||be||bq||br||by||b|||b|||b|||b|||b|v|m|d|[|R|I||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||b|||b|||b|||b|||b|||b|||b|||b|||b|||b|||b|||b|||d|v|m|d|[|R|I|||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||
›|d|||d|||d|||e|||e|||e|||e|||e|||e||e|||e|||e ||edv|m|d|[|R|I||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||ed||el||g|||g|||kr||ky||kz||k|||k|||k|||k|||k|||k|v|m|d|[|R|I||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||
›||k|||k|||k|||k|||k|||k|||k|||k|||n%||n/||nU||n\||q|v|m|d|[|R|I||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||q|||q|||q|||q|||u||u|||y|||y|||y|||y|||z|||z|||~|v|m|d|[|R|I||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||
›||~|||~||||||||||||-|||7|||T|||]|||||||||||||||||||v|m|d|[|R|I|||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||| |||(|||+|||,|||6|||>|||D|||E|||N|||ov|m|d|[|R|I|||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||
›||||o|||w|||x|||||||||||||||M|||Q||||||||||||||||||||v|m|d|[|R|I|||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||'v|m|d||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||
›||||||||||||||	|||@@|||A|||J|||K|||T|||d|||n|||o||||||||woooooooaSSK|||||||||||||||||||
|||||||||@@||0
|||||||||@@||0|||||||||||||||||||||||||||||||||||||||	|||||| |||@@|||i|||j|||||||||||Cwwwwi[SKKKKKK||||||||||||||||||||||
||||||||||| 
||||||||||| ||||||||
|||C|||D|||'|||(|||||||||||T|||d|||e|||o||||||||||||||||wwwwwwwwwwwZR|||||||||||||||||||||||||||||	|||| ||||||| |||||||||||||||
|||||||||||||||||||||||||||#|||$|||2|||FbEEEEEEE4|||||||||||||b
@
