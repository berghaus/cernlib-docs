head     1.1;
access   ;
symbols  ;
locks    goossens:1.1; strict;
comment  @@;


1.1
date     94.03.14.14.50.36;  author goossens;  state Exp;
branches ;
next     ;


desc
@ Initial checkin
@



1.1
log
@Initial revision
@
text
@|4|||||||||||||||F|||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||Q|||||-|||||-|||||;|||||;|||||||||F||||||||||||||||||||||||||||||||||||||||||||=|/||||||||p||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||
›- 177 -
 
For odd m = 2k-1, a natural spline is a spline which is defined by polynomials p(x) of order k-l (rather than 2k-1) in the intervals xl and xn ... cO> i.e.
 
n       2k-1
Snat (x) = p(x) ~ ~ Cj(X-Xj)
 
n   r
~ c jX = O for integer r, O < r < k -1,
 
j=l i
 
p(x) of order k-l or less.
 
A natural spline therefore requires at least k~l knots. The minimum third-order spline (knots x1, X2, X3) would therefore be
 
                          3 X3-X1 3 X2-X1		3
      S(x) = a I bx + c(x-x ~ c(x-x2) ~	C(X-X3)
                             X3-X2 1 X3-X2
 
where all terms above linear cancel out for x 2 X3 .
- 178 -
 
Appendix C
 
COMPARING SPLINES AND POLYNOMIALS
 
The difficulties of polynomial least square fits, as compared to spline fits, are best illustrated in Figs. 2 to 5. Here, in all cases, the function
 
y = exp[-(x-5)21 - exp[-(xl5)2]
 
has been fitted by polynomials of different degrees, and by a thirdorder spline. The fit is performed inside the interval specified by the vertical lines, to the pOillts represented by '~'. The pictures speak for themse 1 ves .
- 181 -
 
Appendix D
 
HARMONIC POLYNOMIALS
 
For p+2 variables xl, ..., xp~2 , there are
 
~ p~nll 1 (p~n~ g(n,p) =           -_ n       (p~l)!n!
 
linearly independent homogeneous polynomials fk of degree n, of which
 
(n~p-1)! h(n,p) = (2n+p) p!n!
 
are harmollic, i.e. they fulfil the (p+2) dimensional Laplace equation
 
P~2 a2fk
 
| = O .
i=~ axi2
 
For p-l (three variables) we therefore have (2nll) homogeneous harmonic polynomials of degree n [Erdelyi~9~1 .
- 182 -
 
DETERMINING THE STATISTICAL SIGNIFICANCE OF EXPERIMENTAL RESULTS
 
F. James
 
CERN, Geneva, Switzerland
 
The results of an experiment are usually expressed as one of the following:
 
1. The measurement of one or more numerical values, parameters of a theory, or
 
2. Evidence that one theoretical hypothesis is more likely to be true than another hypothesis, or
 
3. Evidence for or against one hypothesis being true (without mention of other possible hypotheses).
 
In each of these cases, it is of the greatest importance to be able to understand and present the statistical significance of tl-e results, namely the accuracy of the measurements or the strength of the evidence for or against hypotheses.
 
In these lectures we present the theoretical basis for assigning numerical values to the significance of such results, and discuss the practical application of that theory.
 
In order to squeeze a reasonable amount of significant material into the time alloted, it is necessary to assume some prior knowledge on the part of the reader of these notes, namely a good basic understanding of matrix algebra and differential and integra
›l calculus, and enough background in probability and statistics to understand such terms as expectation, variance, Gaussian distribution, Poisson distribution, and binomial distribution. The reader who feels unsure about such elementary statistical concep
›ts is invited to consult a standard textbook. One such text which is intended for physicists and which will be referred to in these lectures is Eadie et al. (1971).
 
1. ESTIMATION OF PARAMETERS.
 
In this chapter we address ourselves to parameter measurement (for which we use the statistical term estimation), and in particular to the determination of measurement uncertainties, known to physicists as error calculation and known to statisticians as in
›terval estimation.
- 183 -
 
1.1 STATISTICS AS THE INVERSE OF PROBABILITY.
 
Probability theory is a branch of pure mathematics, and although physicists often use it just as they use algebra, it has nothing to say directly about the physical world. The classical problem in probability is: 'Given the probabilities of certain element
›ary events, find the probability of a more complicated combination of such events.' For example, given that events occur randomly in time, with equal probability per unit time, and given that this probability per unit time is known to be, for example, one
› per second, what is the probability of observing exactly three events in an interval of one second? As is well-known, this problem was solved centuries ago by Poisson, and the answer is found by evaluating the Poisson function:
 
P(n) = k" e-k ~ n!
 
where k=1, the expected number of events in the time interval, and n=3, so the probability is 0.0613.
 
Now let us consider the inverse problem: 'Given that a certain number of events have been observed to occur in a given time interval, and given that events are known to occur randomly with constant, but unknown probability k per unit time, what can we say 
›about the value of the constant k?' Here we leave the domain of pure probability and enter the realm of statistics, also a branch of mathematics, but considerably more recent and much closer to experimental science. Already the question posed is more vagu
›e than we would expect for a proper mathematical problem, and as we shall see below, the answer furnished by statistics is open to at least two different interpretations according to whether one adopts the Bayesian or non-Bayesian point of view. All this 
›is not really surprising since the question we are asking (and therefore the mathematics we are going to use to solve it) is basically inductive whereas we are used to mathematics being deductive.
 
Before answering our statistical question and attempting to interpret the meaning of the answer, let us consider a somewhat simpler case where the interpretation is independent of our philosophical viewpoint.
 
1.2 NORMAL THEORY (GAUSSIAN MEASUREMENTS).
 
A large class of phenomena yield variables which are distributed according to a normal or Gaussian distribution.l A discussion of where and why we should expect to meet such a distribution is given in the subsection below; let us assume for the moment that
› we are dealing with a random variable such as the measurement of the length of a table, where the variable is distributed according to:
 
P(x<x'<x~dx) = c exp[-(x-e)2/2s2] dx
 
As used in these lectures, the words normal and Gaussian are completely equivalent.
- 184-
 
where e is the true length of the table and s is the standard deviation of the measurement.
 
The above formula should be read:"The probability of observing (measuring) a value between x and x~dx is proportional to dx and to the exponential of -(x-e)2~2s2." [The normalisation constant c does not interest us here, but it is important as the total pr
›obability for observing any x' must be one.l In order to calculate this probability one must know the true value e, which in practice we can only determine by using measurements x, so the whole process appears to be circular. In fact, although we don't kn
›ow e, we do know a value of x, our measurement, and interestingly enough we notice that the formula is symmetric upon interchange of x and e, so that we can consider that it also gives the distribution of e for a given x.
 
As we will see in more detail below, the precise interpretation of the Gaussian formula will depend on which of the two schools of statistics we choose to follow:
 
1. The classical aPProach is that e is unknown but fixed, and we can talk only about probability distributions for observing x.
 
2. The Bayesian approach allows for distributions of degrees of belief in the value of e.
 
In any case the symmetry of the Gaussian distribution in e and x causes the numerical determination of the statistical significance of the measurement to be exactly the same in the two cases, namely the area under the relevant part of the Gaussian probabil
›ity density curve. For example, the probability that e and x are less than s apart (one standard deviation) is about 68%; less than 2s apart is about 95%, etc.
 
1.2.1 The universalitY of the Gaussian distribution.
 
Physicists are so used to the usual rules-of-thumb connecting standard deviations and significance levels (for example, "two standard deviations is a 5% effect.") that they often do not realize that the usual correspondence is only true for variables (meas
›urements) which are Gaussian distributed. The assumption is that everything is Gaussian distributed, and indeed simple measurements of continuous physical phenomena usually do appear to assume a Gaussian shape, when a series of independent identical measu
›rements are made and the results plotted as a histogram. On the other hand it is easy to see that this cannot be an exact and universal effect, since if it were true for some variable y, it would in general not be true for any function of y such as y2,
 
There is in fact some mathematical grounds for "universal normality", as expressed by the Central Limit Theorem. This theorem states that the sum of n random variables approaches a Gaussian distribution for large n, no matter how the individual variables a
›re distributed. This amazing but well-known theorem, which apparently is contradicted by the reason-
- 185-
 
ing given directly above,2 would explain universal normality if a complex measurement could be considered as the sum of a large number of component measurements. For example, if the inaccuracy in measuring some parameter results from the additive combinati
›on of a large number of smaller elementary inaccuracies, the total inaccuracy should be approximately Gaussian, for identical but independent measurements.
 
1.2.2 Real-life resolution functions.
 
If the sum of random variables is asymptotically Gaussian distributed, even when the individual random variables are not, the sum of probability distributions is in general not at all Gaussian, even if all the component distributions are Gaussian, as long 
›as they have different widths.
 
Let us assume, for example, that the process of measuring the length of a table with some measuring instrument provides Gaussian distributed values of the length, with a given standard deviation s. We make a series of measurements with this instrument, but
› then continue with another instrument having a different accuracy and therefore a different value of s. Now if we look at the combined histogram of all measurements together, we will see a superposition of two Gaussians, with an overall shape which is no
›t at all Gaussian, even though the basic process is Gaussian.
 
This situation is familiar in high energy physics where the resolution function is in general a superposition of Gaussians of different widths Iying in a continuous range corresponding to the fact that the accuracy with which one can measure tl-e momentum 
›and angles of a particle track are continuous functions of the momentum and angles themselves. The resulting resolution function is always more sharply peaked than a Gaussian (due to the measurements with exceptionally small errors) and also has longer ta
›ils than a Gaussian (due to measurements with the largest errors).
 
1.3 CONFIDENCE INTERVALS (CLASSICAL THEORY).
 
The classical theory of confidence intervals allows us to find an interval (a range of parameter values) which, with a given probability, will contain the true value of the parameter being measured. We show here how such intervals can be constructed, at le
›ast in theory, for the most general case.
 
2 The resolution of this paradox resides in the nature of convergence to limits for distributions of random variables. The convergence will be different for different functions of the random variable, even if the limit is the same.
- 186-
 
Let t be an estimate (measurement) of a parameter whose true value (unknown) is ~. The value of t for a given experiment will be a function of the measured data x:
 
t = t(x)
 
For any given value of ~, we can in principle always find the expected distribution (probability density) of t, which we denote by f(t|~). This function describes the behaviour of the measurement apparatus. For the simplest case, it would simply be a Gauss
›ian centered at t=~, with width given by the measurement accuracy of the apparatus. For the most complicated experiments it is necessary to find f numerically by Monte Carlo simulation of the measuring system.
 
Now consider a range of values of t from t1 to t2, denoted by <t1,t2>. Since f(t|~) is known, we can calculate, for any given value of ~, the probability of obtaining a value of t in the range <t~,t2>:
 
n = P(t1<t<t2|~) = S f(t'|~)dt'
 
t1
 
Similarly, for a given value of ~, 0<~<1, it is possible to find values of t1 and t2 satisfying the above relation, namely that the probability of t lying in the range <t1,t2> is n. In fact, there are in general many different ranges which will satisfy thi
›s relation; it can be made unique by requiring in addition that the range be central:
 
t 1            -I Oo
S f(t'|~) dt' = S f(t'|~) dt' = a
-OO            t2
 
and therefore a = (1 ~ 2
 
The above relations, for a given confidence level ~, uniquely define two functions t~(~) and t2(~). Since the functions are only defined implicitly, the actual calculation of function values would in general have to be performed numerically by iteration, b
›ut it is in principle always possible, and may even be quite easy in certain simple but important cases. Thus we can draw two curves in (t,~)-space, representing t1(~) and t2(~). Let us take the ~-axis is horizontal, and the t-axis vertical. Then along an
›y vertical line (constant G), the distance between the two curves represents by construction a probability content (that is, the probability of obtaining an estimate t in that range is for that value of ~.)
 
The usefulness of the diagram comes from the fact that this is also true along a horizontal line: namely, for any given value of t, the probability content of the range of ~ lying between the two curves is also ~. The diagram is constructed vertically, but
› can also be read horizontally. To convince oneself that this is true requires some mental gymnastics and a good understanding of the meaning of the probability content of a range of parameter values in the classical sense, but
- 187 -
 
t2(~)
 
t
t2(o ~
 
t=t(x)
 
tl(o ~
 
i
 
~A	~3B
 
Figure 1: Constructing confidence intervals.
 
it is rigorously true. Since the classical interpretation of confidence intervals can best be understood by opposition to the Bayesian interpretation, this should become clearer after the discussion of Bayesian theory below.
 
1.3.1 Example: confidence intervals for the binomial Parameter.
 
The technique outlined above was used, for example, by Clopper and Pearson to find the confidence limits for the parameter of a binomial distribution. This is the distribution which describes processes where only two kinds of events are possible, and a giv
›en event has a constant probability of being of one kind, independent of the other events. ~Examples: measuring the branching ratio for a particle with only two decay modes; measuring a forward-backward or left-right asymmetryl Then, if the probability of
› one event being of one type is p, the probability of n events out of a total sample of N being of that type is given by the well-kmown binomial formula:
- 188 -
 
|N| n      N-n
 
P(n) = In| p (l-p)
 
where Inl is the binomial coefficient N!~(N-n)!n!
 
1 0
 
			~IDENCE IIELT
			WITH COEFFICIENT 95
 
o
  0	1	"	3 4 ~5 ~ 7	~	9	10
			SCAIlE OF X.
 
Figure 2: Clopper-Pearson confidence intervals
for the binomial distribution
 
This was used to construct (vertically) the diagram of Figure 2, which was in turn used horizontally to solve the inverse problem: Given observed values of N and n, what confidence limits can be placed on the value of p?
 
Clopper and Pearson give elaborate diagrams which allow the reader to construct confidence intervals for essentially any N, n, and ~. Later it was recognized that these values are in fact just those of Fisher's F-distribution with appropriately transformed
› arguments, so that existing tables or subroutines of F can be used to determine exact confidence intervals for p. Additional details concerning this problem and the way in which it arises in the context of physics experiments, may be found in James and R
›oos (1980).
- 189 -
 
1.4 CONFIDENCE INTERVALS (BAYESIAN THEORY).
 
The Bayesian theory of parameter interval estimation, which is not accepted by all statisticians, is based on an extension of the domain of validity of Bayes' theorem beyond what a "classical" statistician would deem proper. We therefore begin this section
› by recalling Bayes' theorem, a fundamental theorem of probability theory.
 
1.4.1 Bayes' Theorem
 
Consider a set of elements, each of which may belong to set A, or to set B, or to neither or to both A and B. Then the probability of a random element belonging to both A and B is the probability of its belonging to A given that it already belongs to B, mu
›ltiplied by the probability of its belonging to B. The same is clearly true with A and B inverted, so that we can write:
 
P~A and B) = P(A|B)P~B) = P(B|A)P(A)
 
This is Bayes' Theorem. It gives a relationship between a conditional probability P(A|B), and the inverse conditional probability P(B|A).
 
P(B|A) = P(A|B)P(B)~P(A)
 
1.4.2 The Bayesian use of Bayes' Theorem
 
We have assumed known the conditional probability density f(t|~), namely the distribution of estimates t we would obtain if the true value of the parameter were known. Our experiment however gives us just the inverse: it provides a value of t and we wish t
›o make a probability statement about ~. Apparently Bayes' Theorem is just what we need since it allows us to express a conditional probability in terms of its inverse. Straightforward application of the theorem to the case of interest g i v e s :
 
P(~lt) = P(tl~) P(~) ~ P(t)
 
(Each of the above "probabilities" is in fact a probability density and should be followed by a d~ or a dx, but these differentials clearly cancel, so that we can work directly with the probability densities.)
 
Now let us examine each of the factors in the above expression:
 
1. P(~|t) is just what we want (says the Bayesian), the probability density for the true value of the parameter ~, given the measured value of t. The "classical" statistician says that although ~ is unknown, it has only one true fixed value, and it does no
›t make sense to talk about the probability of its taking on any value. The Bayesian counters by calling P(~|t) the "degree of belief" in
- 190 -
 
the value ~, and says that as long as the true value is not known, this quantity behaves like a probability.
 
2. P(t|G) is nothing but our old friend f(t|~).
 
3. P(~) presents a serious problem both in interpretation and in practical calculation. This quantity is known as the Prior knowledqe of the parameter G. The Bayesian claims that we nearly always have prior knowledge about the phenomena we study, and that 
›the experimental results will always be interpreted in terms of this knowledge anyway, so why not build it into the results from the beginning? Even the Bayesian admits however to practical difficulties in expressing vague knowledge about ~, and especiall
›y in expressing complete ignorance, a subject to which statisticians have devoted considerable effort. Moreover, the non-Bayesian insists that it must be possible to express the results of an experiment independently of other outside knowledge, but this i
›s apparently impossible in the Bayesian formulation.
 
4. P(t), the a priori probability of observing t, is apparently even more intractable than P(~), but in fact can be reduced to the same problem since we can express it as:
 
~oo
P(t) = ~ P(t|~) P(G) d~
 
1.5 USE OF THE LIKELIHOOD FUNCTION.
 
In practice, Bayesian theory of confidence intervals is not used by physicists, probably because of the problem of subjectivity involved in the prior knowledge, and also because of practical difficulties arising in computation. Similarly, the classical tec
›hnique of construction of exact confidence intervals is applied primarily to the solution of rather general problems (such as the example of 1.3.1) and is rarely used to estimate errors for particular experiments. The reason why these fundamental exact te
›chniques are not used is that there exists a much simpler approximate method for obtaining interval estimates by using the likelihood function directly.
 
Consider the log-likelihood function for the simplest possible experiment: the direct measurement of a quantity (G) using a measuring engine which produces Gaussian-distributed errors of zero bias and known variance a2:
 
ln L = ln f(t|~) = ln| | exp[-(t-~)Z~2~2]
~J2n
 
= -(t-~)2~2~Z ~ const.
- 191-
 
0,5
 
Figure 3. Parabolic log-likelihood for Gaussian measurement.
 
This is the parabola shown in Figure 3. We know by construction that this represents a measurement with standard deviation = a. That is, if ~O is the value of ~ for which ln L has its maximum, then:
 
P(GO-a<~true<no+a) ~ 0.68
 
P(GO-2a<~true<~o~2a) - 0. 95
 
etc.
 
Now if the value of a is not known, it can be measured from the shape of the parabola in either of two ways:
 
t. By measuring the second derivative of the log-likelihood function at its maximum (or in fact anywhere, since it i5 constant). Then:
 
a2 = la2lnL~a~2|-1
 
2. Or by taking a=| ~ 0|, where ~t is either of the two points where
- 192 -
 
InL(~1 ) = lnL(~O) - t~2.
 
We will call this second way the method of MINOS.3
 
For the case at hand, both methods 9 ive the same value of ~. We consider below application of these methods to the more general case of a 1 ikel ihood function arising from any experiment.
 
t.5.1 The second derivative of the loq-likelihood function.
 
Largely for reasons of computational simplicity, this is the most common method of calculating parameter uncertainties directly from the likelihood function. In the case where several parameters are estimated simultaneously the ful 1 error matrix of the pa
›rameters is obtained simply by inverting the second derivative matrix of the log-likelihood function (usually at its maximum). This method is considered an excellent approximation whenever the log-likelihood function is indeed parabolic in shape, (in whic
›h case the second derivative matrix is constant) which in turn will happen whenever there is a large amount of data, small measurement errors, or a model which is a linear function of the parameters to be estimated.
 
t . 5. 2 The method of MINOS
 
This method is much more recent than the second-derivative method; it was probably originated by Sheppey (CERN, unpublished) around t965 and first implemented in a general program MINROS (now obsolete, superceded by MINUIT) shortly afterward. It is believe
›d to be valid (at least to a good approximation) also in the general non-linear case where the loglikelihood function is not parabolic.
 
The justification for its validity for non-parabolic cases lies in the fact that it is invariant to any (even non-linear) transformation of the parameter ~. That is, if the 68% confidence interval as determined by MINOS for parameter ~ is <~ 12>, and we ma
›ke a transformation of variables to ~ ), then the 68% confidence interval in ~ will be <5~1~9$2>~ where ~     ) and 95z=|'(~1z).   Now there must be some (in general non-linear) transformation of ~ which would transform the log-likelihood function to an e
›xact parabola, for which the method is believed to give a very good approximation to the exact interval. And since the method is invariant under such a transformation, it must also give a good approximate answer in the general case, unlike the second-deri
›vative method.
 
3 MINOS is the name of the subroutine in MINUIT which implements this method. Cther references to MINUIT wi 11 be made in these lectures for the berefit of MINUIT users. MINUIT is available from the CERN Program Library and is described in James and Roos (
› 1975) .
- 193-
 
Note that this method will in general yield asymmetric intervals, with different parameter uncertainties in the positive and negative directions. Intervals may also be non-linear in the sense that twostandard-deviation errors [given by lnL(~)=lnL(~0)-2.0] 
›are not necessarily twice as big as one-standard-deviation errors Igiven by 1 n L ( O ~ ) = 1 n L ( ~ o ) - | 5 1 .
 
1.6 COMBINATION AND PROPAGATION OF UNCERTAINTIES.
 
It often happens that one wants to know the uncertainty of a certain value which is not measured directly but is a known function of other quantities whose uncertainties are known. For example, we may estimate the mass of the ,u lepton using the mass of th
›e n meson (and its uncertainty) and the mass difference Tr-,u, with its uncertainty. Physicists call this propagation of error, since the error in the final result arises from combining the errors of the two component values. It can be considered simply a
›s a transformation of variable rather than a true statistical problem, but it so often arises in a statistical context that it is appropriate to discuss it here.
 
1.6.1 The sum or difference of two variables.
 
It follows directly from the definition of expectation and variance that the expectation of the sum of two independent random variables is the sum of the expectations of the variables, and the variance of the sum is the sum of the variances. (It is importa
›nt to note that this is true only when the two component variables (measurements) are independent. ) This means that the standard deviation of the sum (or difference) is the square root of the sum of the squares of the individual standard deviations. As l
›ong as the individual variables are independent and their variances are finite, this is an exact result for any number of measurements and for any distribution of deviations. If, in addition, the individual measurements are Gaussian, the sum or difference
› will also be Gaussian, and in fact -- by the Central Limit Theorem -- the sum or difference will always be more Gaussian than the individual distribut i ons .
 
1.6.2 Local theory, or the ProPaqation of small errors.
 
Apart from the simplest case of sums or differences, the exact calculation of uncertainties of transformed variables can be extremely complicated and highly dependent on the exact distributions involved. For this reason one usually uses only the local prop
›erties of the distributions around the estimated parameter values, which is usually an excellent approximation, especially when errors are indeed small. Thus one linearizes the transformations by using only the first derivatives of the new variables with 
›respect to the old variables:
- 194 -
 
aR     aR
= ~      V
ii a~; aoj
 
where R=R( ~ 2,...) is the new variable and Vjj is the variance-covariance matrix of the component variables ~. The three dots indicate that there are higher order terms containing higher derivatives of R with respect to ~, but the usual linear approximati
›on is to neglect the higher terms and keep only the sum given here.
 
1.6.3 Error on the ratio of two continuous variables.
 
If x and y are two independent variables with variances aX2 and ay2~ then tstraightforward application of the linear approximation above gives for the variance of the ratio R=x~y:
 
(aR~R)2 = ~X2~x2 ~ ~y2~y2
 
This is a well-known rule-of-thumb, often used by physicists. It is interesting to see how close it is to the exact answer, assuming x and y to be Gaussian-distributed variables. The shocking answer is that the exact variance of R in this case is infinite!
› At first glance, hardly a good approximation. In fact, as long as aX<<|x| and ay<<lyl the approximation is good locally, and gives about the right width to the distribution of R, but it underestimates the extreme tails of the distribution which cause the
› integrated square deviation of R to diverge. The approximation is in some sense closer to what the physicist really wants to know than the exact answer, and indeed the exact 68~o confidence interval for R (which is of course finite) is close to that give
›n by the linear approximation. The physicist should however be aware of the fact that the distribution of R deviates strongly from a Gaussian in the tails, especially when the errors in x and y are not small compared with the values of x and y (especially
› y).
 
1.6.4 Error on the ratio of two discrete variables.
 
An important case where the local approximation does not work is where small samples of discrete data are involved. Fortunately, exact methods are often relatively easy to apply for these cases, so good solutions can be found, but the usual approximations 
›must be avoided.
 
Consider the measurement of a branching ratio for the decay of an unstable particle with only two decay modes. We observe, in a given time interval, n decays of one mode and N-n decays of the other. The branching ratio may be estimated as R~n~N, and it is 
›tempting to estimate the uncertainty on R by combining uncertainties of Jn and JN using the formula given above for the error of a ratio. Two difficulties arise: (l) n and N are not independent, and (2) the linear approximation is very poor when n and N a
›re small (say less than 10). The exact solution of this problem follows from the fact that the distribution
- 195-
 
involved is really binomial (since there are only two outcomes for a decay). A complete treatment of this problem including examples of how it arises in physics experiments (and how some experimenters have published incorrect error analyses by using the ap
›proximation when it was not justified) is given in James and Roos (1980).
 
1.6.5 The propaqation of larqe errors.
 
The question now arises of what to do in the general case for continuous variables when the linear approximation for error propagation is suspected of being poor. Straightforward calculation of the distribution of the new variable R involves complicated in
›tegrals over the component distributions which, even if they are independent Gaussians, quickly become intractable, and one must resort to numerical calculations even in relatively simple cases.
 
One such case came up recently in the analysis of an experiment by Reines, Sobel, and Pasierb which gives evidence for the instability of the neutrino. This result is of the greatest importance in high energy physics since it has generally been believed th
›at all neutrinos were massless and could not decay. In view of the consequences of neutrino decay, it is necessary to determine the significance of these results accurately. The final result of the experiment is the measurement of the ratio of two cross s
›ections, let us call this R. Expressed in terms of the elementary quantities measured in the experiment, it can be written as:
 
R =		d		k2d
 (b-c) - 2 (1 - ) a
k2e		ke
 
where		a = 3.84 + 1.33
b = 74 + 4
c = 9.5 + 3
d = 0.112 + 0.009
e = 0.32 + 0.002
k = 0.89
 
Straightforward application of the linear approximation gives:
 
R ~ 0.191 + 0.073
 
But theoretical calculations show that the neutrino is unstable if R is less than about 0.42. Therefore, based on approximate error analysis, the result appears to be very significant: 3.2 standard deviations or about one chance in a thousand that the neut
›rino is stable.
- 196-
 
However, two of the elementary quantities have large errors, and two quantities enter into the formula twice, producing correlations. In addition, there are several fractions, which we have seen cause nonGaussian distributions, so let us try to calculate t
›he exact confidence intervals for R. The easiest (and perhaps the only) way to do this is by Monte Carlo. Choose values of a,b,c,d,e randomly according to the appropriate Gaussian distributions (we will be optimistic and assume that at least the elementar
›y measurements are Gaussian with known variances), and plot the resulting values of R. The FORTRAN program to do this is so simple that I include it here (Calls to subroutines beginning with H are for the HBOOK histogramming package; NORRAN is a Gaussian 
›random number generator; all subroutines called here are from the CERN Program Library):
 
	PROGRAM REINES(INPUT,OUTPUT)
C	    CALCULATION OF ERROR ON NEUTRAL TO CHARGED CURRENT
C	    NEUTRINO INTERACTIONS, D'APRES REINES AND ROOS.
 
C SET UP HISTOGRAM OF R CALL HBOOKl(l,lOH N OVER D , 50 ,0.,0.5,0.)
 
C FILL HISTOGRAM BY LOOPING OVER RANDOM SAMPLES OF R
DO 100 I= 1, 10000
CALL NORRAN(XN)
XN = XN*1.33 + 3.84
CALL NORRAN(X112)
X112 = X112 * .009 ~ 0.112
CALL NORRAN(X74)
X74 = X74 * 4. + 74.
CALL NORRAN(X95)
X95 = X95 * 3. + 9.5
CALL NORRAN(X32)
X32 = X32 * 0.02 + 0.32
X89 = 0.89
Dl = X112*(X74-X95)~(X89*X32)
D2 = 2.0 * XN * (1.0 - (X89*X112~X32))
XXX = XN~(Dl-D2)
CALL HFILL (l,XXX)
100 CONTINUE
 
C ASK FOR PRINTING OF HISTOGRAM, ~ITH INTEGRATED CONTENTS
CALL HINTEG(l, 3HYES)
CALL HISTDO
 
STOp
END
 
The histogram showing the distribution of the 10000 Monte Carlo values of R is shown in Figure 4. Those of you familiar with the reading of HBOOK output will quickly find the significant number, namely the number of entries falling above 0.42. This is almo
›st 4%, so that the true significance of the result is only 4% instead of the apparent 0.1%. Notice also the skew, non-Gaussian distribution of R.
- 197 -
 
     500
     480										 - I-
     460										 I ~
     440										-I		I
     420
     400									I			I-
     380
     360
     340
     320								I					I-
     300
     280
     260						I								1-
     240						I								 I-
     220						I								  I-
     200
     180					I										I--
     160					I										  I-
     140
     120				I											   I---
     100
      80			I													I-
      60
      40
      20
 
CHANNELS	10	0					1				2				3		4	5
1	12345678901234567890123456789012345678901234567890
 
CONTENTS 100	1112233444444444333222221111111
10	12358238693201265965493168531876401089754543322221
1.	90992724101391557441788476794339467366374772477~61
 
INTEGRAT1000	11122334445566677778888889999999999999999
100	12346925826150594826924792357890123455666777888
10	13731476214678173294982409580852779988515159257034
1.	99879682334767274823086073093698285840304180418401
 
LOW-EDGE	 1.	1111111111222222222233333333334444444444
*10** 1	0	01234567890123456789012345678901234567890123456789
 
ENTRIES = 10000   UNDERFLOW = 23   OVERFLOW = 136 MEAN VALUE = .2009E+00      R . M . S =.9219E-01
 
Figure 4. Distribution of R for experiment of Reines et al.
- 198 -
 
1.7 MULTIPARAMETER ESTIMATION.
 
In addition to the difficulties described above, a special class of problems arise in interpreting errors when there are more than one free parameters. These problems are quite separate from those described above and are really much simpler in principle, a
›lthough in practice confusion often arises.
 
1.7.1 The Error Matrix
 
The error matrix, also called the covariance matrix, is the inverse of the second derivative matrix of the (log-likelihood or chisquare) function with respect to its free parameters, usually assumed to be evaluated at the best parameter values (the functio
›n minimum). The diagonal elements of the error matrix are the squares of the individual parameter errors, includinq the effects of correlations with the other parameters.
 
The inverse of the error matrix, the second derivative matrix, has as diagonal elements the second partial derivatives with respect to one parameter at a time. These diagonal elements are not therefore coupled to any other parameters, but when the matrix i
›s inverted, the diagonal elements of the inverse contain contributions from all the elements of the second derivative matrix, which is 'where the correlations come from'.
 
Although a parameter may be either positively or negatively correlated with another, the effect of correlations is always to increase the errors on the other parameters in the sense that if a given free parameter suddenly became exactly known (fixed), that
› would always decrease (or at least not change) the errors on the other parameters. In order to see this effect quantitatively, the following procedure can be used to 'delete' one parameter from the error matrix, including its effects on the other paramet
›ers:
 
1. Invert the error matrix, to yield the second-derivative matrix.
 
2. Remove the row and column of the inverse corresponding to the given parameter, and
 
3. Re-invert the resulting (smaller) matrix.
 
This reduced error matrix will have its diagonal elements smaller or equal to the corresponding elements in the original error matrix, the difference representing the effect of knowing or not knowing the true value of the parameter that was removed at step
› two. This procedure is exactly that performed by MINUIT when a FIX command is executed. Note that it is not reversible, since information has been lost in the deletion. The MINUIT commands RESTORE and RELEASE therefore cause the error matrix to be consid
›ered lost and it must be recalculated entirely.
- 198 -
 
1.7 MULTIPARAMETER ESTIMATION.
 
In addition to the difficulties described above, a special class of problems arise in interpreting errors wl-en there are more than one free parameters. These problems are quite separate from those described above and are really much simpler in principle, 
›although in practice confusion often arises.
 
1.7.1 The Error Matrix
 
The error matrix, also called the covariance matrix, is the inverse of the second derivative matrix of the (log-likelihood or chisquare) function with respect to its free parameters, usually assumed to be evaluated at the best parameter values (the functio
›n minimum). The diagonal elements of the error matrix are the squares of the individual parameter errors, includinq the effects of correlations with the other parameters.
 
The inverse of the error matrix, the second derivative matrix, has as diagonal elements the second partial derivatives with respect to one parameter at a time. These diagonal elements are not therefore coupled to any other parameters, but when the matrix i
›s inverted, the diagonal elements of the inverse contain contributions from all the elements of the second derivative matrix, which is 'where the correlations come from'.
 
Although a parameter may be either positively or negatively correlated with another, the effect of correlations is always to increase the errors on the other parameters in the sense that if a given free parameter suddenly became exactly known (fixed), that
› would always decrease (or at least not change) the errors on the other parameters. In order to see this effect quantitatively, the following procedure can be used to 'delete' one parameter from the error matrix, including its effects on the other paramet
›ers:
 
1. Invert the error matrix, to yield the second-derivative matrix.
 
2. Remove the row and column of the inverse corresponding to the given parameter, and
 
3. Re-invert the resulting (smaller) matrix.
 
This reduced error matrix will have its diagonal elements smaller or equal to the corresponding elements in the original error matrix, the difference representing the effect of knowing or not knowing the true value of the parameter that was removed at step
› two. This procedure is exactly that performed by MINUIT when a FIX command is executed. Note that it is not reversible, since information has been lost in the deletion. The MINUIT commands RESTORE and RELEASE therefore cause the error matrix to be consid
›ered lost and it must be recalculated entirely.
- 199 -
 
1.7.2 MINOS with several free Parameters
 
The MINOS algorithm is described in some detail in the MINUIT long-write-up and will not be repeated here, but we will add some supplementary 'geometrical interpretation' for the multidimensional case (which is the usual case -- in fact, early versions of 
›MINOS had a bug which prevented them from working in the one-parameter case because it had not occurred to the authors that anybody would use it for only one parameter!).
 
Let us consider that there are just two free parameters, and draw the contour line connecting all points where the function takes on the value Fmjn I UP. (The CONTOUR command will do this for you from MINUIT). For a linear problem, this contour line would 
›be an exact ellipse, the shape and orientation of which are described in Eadie et al, p.196 (fig. 9.4). For our problem let the contour be as in Figure 5 below. If MINOS is requested to find the errors in parameter one (the x-axis~, it will find the extre
›me contour points A and B, whose x-coordinates, relative to the x-coordinate at the minimum (X), will be respectively the negative and positive MINOS errors of parameter one.
 
looooooo
 
			oo	ooooo
		o			ooo
	oo					B
oo					oo
oo						oo
o						oo
oo						oo
ooo						o
oo						o
 
I o						o						param. 1
 
1||									o
 
A								o
 o								o
 o							o
oo					oo
oo			oo
ooo	ooo
o o o oo
 
Figure 5 - MINOS errors for parameter 1
- 200 -
 
1.7.3 Probability content of confidence reqions
 
For an n-parameter problem MINOS performs minimizations in (n-l) dimensions in order to find the extreme points of the hypercontour of which a two-dimensional example is given in Figure 5, and in this way takes account of all the correlations with the othe
›r n-l parameters. However, the errors which it calculates are still only single-parameter errors, in tl-e sense that each parameter error is a statement only about the value of that parameter. This is represented geometrically by saying that the confidenc
›e region expressed by the MINOS error in parameter one is the cross-hatcl-ed area of Figure 6, extending to infinity at both the top and bottom of the figure.
 
o o o o o o o ~
			  oo ~		ooooo~
			 o ~		o o o
 
		  oo ~			oo
		oo ~			oo~
 
	   oo ~				oo
	ooo ~				o ~
 
1~o ~				o ~		param. 1
 
~o o ~			  o o
o o ~		o o
 ooo ~	 ooo
o o o o o ~
 
Figure 6 - MINOS error confidence region for parameter 1
 
If UP is set to the appropriate one-std.-dev. value, then the precise meaning of the confidence region of Figure 6 is: "Tl-e probability that the true value of parameter one lies between A and B is 68.3~" (the probability of a Normally-distributed paramete
›r lying within one std.-dev. of its mean). That is, the probability content of the crosshatched area in Figure 6 is 68.3~. No statement is made about the simultaneous values of the other parameter(s), since the cross-hatched area covers all values of the 
›other parameter(s).
 
If it is desired to make simultaneouslv statements about the values of two or more parameters, the situation becomes cosiderably more com-
 
|6||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||
|||
||||||||||||	||||||||||3|||;|||<|||F|||V|v|m|d[|R|I|||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||V|||`|||a|||c|||d|||g|||h|||o|||p|||r|||s|||~|||fv|m|d|[|R|I|||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||f|||o|||
›||||||||{||||||||||||||||||||||||||||||||v|m|d|[|R|I||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||v|m|d|[|R|I||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||
›||||||||| |||(|||||||||||||||"|||#|||%|||&v|m|d|[|R|I|||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||&|||)|||*|||2|||3|||?||%P||%Y||%Z||%d||%e||%n||%|v|m|d|[|R|I||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||%|||%||
›|%|||%|||%|||%|||&|||&|||&|||&[||&^||)|v|m|d[|R|I||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||)|||)|||)|||)|||)|||+|||+ ||+!||+*||+,||+5||+6vm|d|[|R|I||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||+6||+<
›||2|||2"||8|||8|||8|||8|||8|||8|||8|||8|||8|v|m|d|[|R|I||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||8|||8|||8|||8|||8|||8|||<|||<|||?]||?g||?h||?q||?sv|m|d|a|X|O|||||||||||||||||||||||||||||||||||@@||||||||||||||||||||||||||||||?s||?
›{||?|||?|||@@|||@@|||@@|||@@|||B|||B|||B|||B|||C3v|m|d|[|R|I||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||C3||C6||C7||C?||C@@||CC||CD||CF||CG||CM||CN||CU||I(v|m|d|[|R|I||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||I(||
›I-||I.||I2||I7||K|||K|||K|||K|||L|||L|||L|v|md|[|R|I||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||L|||L|||L|||L|||L|||L|||L|||L|||TR||TU||TV||T\||T]v|m|d|[|R|I||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||T]|
›|Tg||Th||Tj||Tk||Tn||To||T}||T~||T|||Wj||Wm||Wnv|m|d|[|R|I||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||Wn||Wt||Wu||Ww||Wx||W}||^|||^|||^|||^|||^|||^|||^|v|m|d|[|R|I||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||^|
›||^|||^|||^|||ax||a{||a|||a|||a|||a|||a|||a|||a|v|m|d|[|R|I||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||a|||a|||a|||a|||a|||a|||b||b|||eF||eK||eL||eR||eTv|m|d|[|R|I||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||e
›T||eV||eW||eZ||e[||ef||eg||ei||ej||eo||ep||ev||iv|m|d|[|R|I||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||i||i|||i|||i|||i|||i|||i|||i|||i|||i!||i"||i%||i&v|m|d|[|R|I||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||
›i&||i0||i1||i:||n|||n|||n|||n|||n|||n"||n#||n(||n)v|m|d|[|R|I||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||n)||n+||n,||n/||n0||n8||n9||nB||s|||s|||s ||s+||s,v|m|d|[|R|I|||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||
›|s,||s.||s/||s4||s5||s;|||||||||||||||||||||||"|||#v|m|d|[|R|I|||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||#|||(|||)|||/||||||||||||||||||||||||||||||||||||v|m|d|[|R|I||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||
›||||||||||||||||||||||||||||||||||||||||||||||||||||v|m|d|[|R|I|||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||`|||i|||j|||m|||n|||u|||v|||x|||y|||||||^v|m|d|[|R|I||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||
›|||^|||a|||[|||`|||a|||e|||f|||m|||n|||r|||s|||}||||v|m|d|[|R|I||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||
|||||||||||||||Fv|m|d|[|R|I|||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||	|||||||||||||||||||||||||||||||||||||||||||"|||#||||||||||||wooooooooooooooo^||||||||||||||`#p|||||||||||||||||||||||||||||||||)|||*|||_|||h|||i|||t|||u
›|||||||||||-|||.|||N|||O|||3|||<|||=nnnnf^^^^^^^^^^^^||||||||||||||||||||||||||||||`#p|||||||||=|||H|||I|||^|||_||||||||||||||||||||||||||)|||*|||q|||r|||{||||||||||||||||||||||||||||wwwwwwwwwwwwwwwwwwwwww|||||||||||||||||||||H|||I|||R|||S|||m|||n||||||||
›||||||||||m|||n||||||||||	|||	|||q||r||
|||
|||
4||
5wwwwwwwwwwwwwwwwwwwwww||||||||||||||||
5|||H|||Q|||R|||||||||||e|||f|||y|||z|||||||||||||||||||v|||w|||||||||||,|||-|||R|||S||||wwwwwwwwwwwwwwwwwwwwww||||||||||||||||||||||||||||||||
|||||||||||||||||||||||||||j|||k||||||||||A|||B||">||"?||#|||#|||#|||%Iwwwwwwwwwwwwwwwwwwwwww||||||||||||||||%I||%J||%p||%q||&|||&|||(|||(|||+|||+|||+?||+@@||,j||,k||-S||-[||-\||.|||.|||.||.|||/|||/|wwwwwwwwwwwwwwwwwwwwww||||||||||||||||/|||0|||0|||0|||0||
›|0|||0|||2$||2%||2:||2\||2n||2o||2|||2|||5W||5X||7<||7E||7F||7L||7M||7Owwwwwwwwwwwwwwwwwwwwww||||||||||||||||7O||7V||7W||7^||7_||7f||7g||7i||7j||7q||7r||7|||7|||8|wwwwwwwwi[SKK||||||||||||||||||||||
||||||||||||P
||||||||||||P||||||||
||8|||8|||8|||8|||;\||;e||;f||;u||;v||;|||;|||;|wwwwwwwwwwZ||||||||||||||||||||||||||||||||||||||||||||`||||||||@@|||||||||||||||||;|||;|||;|||;|||;|bZR5||||||||||||||||||||||||||||||||||||||||||`||||||||@@|||||||||||||||||||||||||||||||||||`||||||||@@|||||||
›||;|||;|||;|||;|||<|||<|bbZ= ||||||||||||||||`||||||||@@|||||||||||||||||||`||||||||@@|||||||||||||||||||||||||||`||||||||@@|||||||||<|||<|||<G||<e||<f||=C||=D||?O||?X||?Y||?|||?|||@@|||@@|||@@|||@@|bZRRRRRRRRRRRRR||||||||||||||||||||||||||||||||`||||||||@@||||||
›|||@@|||Ba||Bb||B|||B|||C|||C|||C,||C-||CV||CW||EN||EO||Ek||El||F>||F?||F|||F|||H	||H|||H|||H|wwwwwwwwwwwwwwwwwwwwww||||||||||||||||H|||H|||H|||H|||K|||K|||L|||L|||L|||L|||L|||L|||L|||Om||On||PJ||PK||Pu||Pz||P{||P|||P|||P|wwwwwwwwwwwwwwwwwwwwww||||||||||||
›||||P|||P|||P|||P|||P|||Q|||Q|||Q|||Q|||Q|||Q|||Q|||Q|||RR||RS||R|||R|||R|||R|||S4||S=||S>||SXwwwwwwwwwwwwwwwwwwwwww||||||||||||||||SX||SY||S|||S|||TK||TL||T|||T|||W`||Wa||W~||W|||Y|||Y|||\|||\|||]|||]'||](||^|||^|||^|||^|wwwwwwwwwwwwwwwwwwwwww|||||||||||
›|||||^|||aq||ar||a|||a|||e?||e@@||ex||ey||g|||g|||g|||g|||g|||g|||g|||i|||i|||i<||i=||i|||i|||jwwwwwwwwwwwwwwwwwwwwww||||||||||||||||j||j
||n|||n|||nD||nE||oV||oW||q|||q|||q|||s|||s|||s=||s>||u|||u|||w|||w|wwwwwwwwwwwwwwwwww||||||||||||||||||||||||||||||||||||w|||w|||w|||w|||w|||w|||w|||w|||w|||w|||x|eKKKKKKC;;||||||||||||||||||||||||||||||||||p |||| ||||@@|||||||||||||p |||| ||||@@||x|||x|||x
›D||xE||xW||xX||yh||yp||yq||||||||||||||} ||}V||}W||}|wwwwwwwwwwi[[[S||||||||||||
||||||||`|||p
||||||||`|||p|||||||||||}|||}|||}|||}|||}|||~|||~
@
